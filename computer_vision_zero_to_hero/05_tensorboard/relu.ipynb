{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6212c85-79d7-485d-a729-c1f743a4a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61d19a9-721e-44df-8c26-62036c6df4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9c5627-defb-4261-93ab-2eea82f82a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948af954-1a48-492e-bfbe-2a378f221715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image(image, label):\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.show()\n",
    "show_image(X[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcca71b1-6661-4907-996e-0e76c825adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "804e1a94-a924-484e-91a1-95f9a7073806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='mps:0'), 7680)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = act1 < 0.0\n",
    "mask.float().sum(), act1.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "270069ce-1c32-4f93-9374-40205ed77513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1.shape\n",
    "s = act1.sum(dim=0)\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b99df5eb-2171-41c5-bd6f-58f5332c52e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.1413e-01, -1.3174e-01, -1.7960e-01,  8.1790e-02,  2.8773e-01,\n",
       "           7.6665e-02, -5.2325e-02, -5.4442e-02, -2.3105e-01, -2.5383e-01],\n",
       "         [ 2.7132e-01, -1.9914e-01, -1.7635e-01,  1.7164e-01,  2.8408e-01,\n",
       "           7.9833e-02,  6.5947e-02, -8.1150e-02, -1.6381e-01, -4.1387e-01],\n",
       "         [ 2.0279e-01, -2.0074e-01, -1.4522e-01,  7.1057e-02,  1.5798e-01,\n",
       "           1.3737e-01,  5.5082e-02, -3.3226e-02, -1.4558e-01, -2.1639e-01],\n",
       "         [ 3.3408e-01, -2.4143e-01, -7.0178e-02,  3.2176e-01,  3.2919e-01,\n",
       "           4.0479e-02,  4.3348e-02, -3.5875e-02, -9.5553e-02, -4.3689e-01],\n",
       "         [ 2.4855e-01, -1.5805e-01, -1.3531e-01,  2.6214e-01,  3.6463e-01,\n",
       "           1.0297e-01,  1.9478e-03, -1.2961e-01, -1.4957e-01, -3.1446e-01],\n",
       "         [ 2.2038e-01, -2.1186e-01, -1.4744e-01,  7.2944e-02,  2.0147e-01,\n",
       "           1.6092e-01,  4.4065e-02, -3.2001e-02, -1.2499e-01, -2.3656e-01],\n",
       "         [ 2.8013e-01, -2.4514e-01, -1.1981e-01,  2.5401e-01,  4.1937e-01,\n",
       "           7.6577e-02,  4.0452e-02, -6.4902e-02, -1.2136e-01, -3.9065e-01],\n",
       "         [ 2.7197e-01, -2.9555e-01, -1.1318e-01,  3.2225e-01,  3.2943e-01,\n",
       "           1.2120e-01,  4.2451e-02,  3.6081e-02, -1.8479e-01, -4.7772e-01],\n",
       "         [ 2.8650e-01, -2.4423e-01, -1.1402e-01,  2.4441e-01,  3.3668e-01,\n",
       "           8.4788e-03,  1.0849e-01,  3.2762e-02, -1.5349e-01, -3.7582e-01],\n",
       "         [ 3.1287e-01, -2.4363e-01, -7.3778e-02,  1.9299e-01,  3.4187e-01,\n",
       "           1.2009e-01,  4.0736e-02,  2.1569e-02, -1.6340e-01, -4.7739e-01],\n",
       "         [ 2.8021e-01, -1.6533e-01, -1.3675e-01,  3.1480e-01,  3.3953e-01,\n",
       "          -8.4599e-02, -2.2065e-02, -9.0421e-02, -1.4651e-01, -3.2054e-01],\n",
       "         [ 2.9434e-01, -2.6605e-01, -1.5338e-01,  2.9071e-01,  4.3011e-01,\n",
       "           2.5454e-02, -4.4581e-02, -9.0061e-02, -1.8546e-01, -4.0926e-01],\n",
       "         [ 2.9389e-01, -2.1543e-01, -2.3590e-02,  2.0581e-01,  3.1734e-01,\n",
       "           7.6691e-02,  6.8989e-02, -4.7910e-02, -8.5594e-02, -4.2838e-01],\n",
       "         [ 3.6829e-01, -3.1065e-01, -7.0843e-02,  2.9181e-01,  3.5397e-01,\n",
       "           3.8858e-02,  5.3075e-02,  3.8626e-03, -1.0051e-01, -3.9757e-01],\n",
       "         [ 2.9235e-01, -1.9382e-01, -1.3814e-01,  1.1941e-01,  2.7221e-01,\n",
       "           1.5760e-01,  8.9858e-02, -1.1491e-01, -1.4560e-01, -3.0474e-01],\n",
       "         [ 2.8271e-01, -2.1812e-01, -9.5355e-02,  2.2766e-01,  3.1083e-01,\n",
       "           6.8300e-02, -1.1679e-02,  1.9592e-03, -1.0909e-01, -3.2325e-01],\n",
       "         [ 2.7953e-01, -2.3816e-01, -1.0339e-01,  2.9552e-01,  4.5706e-01,\n",
       "           9.9091e-02,  4.0050e-02, -9.9616e-02, -1.1825e-01, -4.1267e-01],\n",
       "         [ 2.4914e-01, -1.0068e-01, -1.1788e-01,  1.6561e-01,  2.9998e-01,\n",
       "           1.9410e-02, -1.0580e-01, -2.2554e-02, -2.2288e-01, -3.2216e-01],\n",
       "         [ 3.1300e-01, -1.9070e-01, -6.7974e-02,  3.0474e-01,  4.7631e-01,\n",
       "          -5.6675e-04, -3.5167e-02, -3.1873e-02, -1.9724e-01, -6.0693e-01],\n",
       "         [ 2.6887e-01, -2.2919e-01, -2.1288e-02,  2.8083e-01,  3.2816e-01,\n",
       "           2.6973e-02,  7.8785e-02, -7.1849e-02, -8.4476e-02, -3.7132e-01],\n",
       "         [ 3.4881e-01, -2.1128e-01, -8.8519e-02,  2.0215e-01,  3.7135e-01,\n",
       "           9.2358e-02,  7.7144e-02, -3.0192e-02, -1.3383e-01, -4.6080e-01],\n",
       "         [ 2.6622e-01, -2.3809e-01, -1.2501e-01,  3.0009e-01,  4.1978e-01,\n",
       "           4.9998e-02,  2.9778e-02, -6.4665e-02, -1.2668e-01, -4.1333e-01],\n",
       "         [ 3.0473e-01, -1.9943e-01, -1.1537e-01,  2.0571e-01,  3.5918e-01,\n",
       "           6.1870e-02,  3.1573e-02,  3.6713e-03, -2.0002e-01, -3.7067e-01],\n",
       "         [ 2.3827e-01, -1.9175e-01, -7.7019e-02,  2.1366e-01,  2.2331e-01,\n",
       "           9.4746e-02,  5.2026e-02, -4.0019e-02, -1.5083e-01, -3.6542e-01],\n",
       "         [ 1.6259e-01, -1.5282e-01, -8.1582e-02,  2.1109e-01,  2.7739e-01,\n",
       "           1.0755e-01,  2.1864e-03, -5.4028e-02, -1.5725e-01, -3.3008e-01],\n",
       "         [ 3.9559e-01, -2.4122e-01, -1.4566e-01,  2.8281e-01,  4.2698e-01,\n",
       "          -4.8739e-02,  4.8394e-02,  3.3897e-02, -1.7344e-01, -4.8120e-01],\n",
       "         [ 2.4113e-01, -1.7261e-01, -1.3411e-01,  2.0286e-01,  2.2970e-01,\n",
       "           1.0697e-01, -2.0377e-02,  1.5476e-02, -1.4498e-01, -3.2055e-01],\n",
       "         [ 3.1959e-01, -1.4951e-01,  1.6552e-03,  3.2666e-01,  3.9033e-01,\n",
       "           4.9228e-02,  7.3600e-02, -9.2299e-02, -8.2267e-02, -4.0360e-01],\n",
       "         [ 3.6229e-01, -2.1549e-01, -8.9389e-02,  3.1679e-01,  3.1093e-01,\n",
       "          -2.9566e-03,  8.4524e-02, -2.9862e-02, -1.2285e-01, -4.4106e-01],\n",
       "         [ 2.7538e-01, -1.6510e-01, -1.3501e-01,  1.2875e-01,  2.5716e-01,\n",
       "           1.2463e-01,  1.0082e-01, -1.5775e-01, -1.4931e-01, -3.0251e-01],\n",
       "         [ 2.1890e-01, -1.7197e-01, -1.1295e-01,  1.6825e-01,  3.7616e-01,\n",
       "           6.5597e-02, -6.6195e-02,  1.3909e-02, -1.8674e-01, -4.0396e-01],\n",
       "         [ 1.9121e-01, -1.4740e-01, -1.1014e-01,  1.4355e-01,  2.0030e-01,\n",
       "           1.3138e-01,  1.9189e-02, -1.0828e-01, -1.6490e-01, -2.7395e-01],\n",
       "         [ 2.9375e-01, -2.2015e-01, -1.1914e-01,  3.3995e-01,  3.8905e-01,\n",
       "           6.4990e-02, -2.4799e-02, -6.7426e-02, -1.9368e-01, -4.8869e-01],\n",
       "         [ 3.5996e-01, -2.1893e-01, -6.6250e-02,  2.6935e-01,  3.0655e-01,\n",
       "           1.2598e-02,  4.4803e-02, -1.0032e-01, -1.5464e-01, -4.0558e-01],\n",
       "         [ 2.3434e-01, -1.3212e-01, -9.6691e-02,  4.3567e-02,  2.8099e-01,\n",
       "           1.1891e-01,  2.8683e-02,  8.4990e-03, -2.4980e-01, -3.3372e-01],\n",
       "         [ 2.0810e-01, -1.4771e-01, -1.7744e-01,  2.5512e-01,  3.4241e-01,\n",
       "           2.4025e-02,  1.2611e-02,  2.5312e-02, -2.6234e-01, -3.3460e-01],\n",
       "         [ 2.4359e-01, -1.4506e-01, -1.5497e-01,  1.3710e-01,  2.2664e-01,\n",
       "           1.0658e-01, -4.0962e-02,  7.0115e-03, -2.3432e-01, -3.2984e-01],\n",
       "         [ 2.6126e-01, -1.8158e-01, -1.2167e-01,  1.2059e-01,  2.6184e-01,\n",
       "           1.4902e-01,  9.8114e-02, -1.2907e-01, -1.5795e-01, -2.7319e-01],\n",
       "         [ 2.1824e-01, -2.1955e-01, -1.6003e-01,  1.9270e-01,  1.6145e-01,\n",
       "           1.1827e-01,  4.8790e-02,  5.9551e-03, -1.3643e-01, -3.2479e-01],\n",
       "         [ 2.9454e-01, -2.0191e-01, -1.5827e-01,  6.6849e-02,  2.6038e-01,\n",
       "           1.7075e-01,  8.1268e-02, -5.5097e-02, -1.1821e-01, -3.3412e-01],\n",
       "         [ 1.7920e-01, -1.9076e-01, -1.3739e-01,  1.0435e-01,  1.0425e-01,\n",
       "           1.3869e-01,  6.5238e-02, -7.3671e-02, -1.7298e-01, -2.1113e-01],\n",
       "         [ 2.1279e-01, -2.0161e-01, -1.4103e-01,  1.6354e-01,  2.0456e-01,\n",
       "           1.5640e-01,  2.2885e-02,  4.1289e-02, -1.6287e-01, -3.2879e-01],\n",
       "         [ 2.6587e-01, -2.1401e-01, -2.2405e-02,  2.3434e-01,  3.7851e-01,\n",
       "           1.4358e-02,  1.4819e-02, -8.3435e-02, -1.1114e-01, -3.6556e-01],\n",
       "         [ 2.7741e-01, -1.9648e-01, -8.4192e-02,  1.3710e-01,  1.9561e-01,\n",
       "           4.5750e-02,  7.2806e-02, -2.4695e-02, -2.0074e-01, -3.7748e-01],\n",
       "         [ 2.4643e-01, -1.8439e-01, -1.5785e-01,  1.5474e-01,  3.3567e-01,\n",
       "           6.8375e-02,  2.8219e-02, -7.7930e-02, -1.7399e-01, -3.8531e-01],\n",
       "         [ 2.6615e-01, -1.6784e-01, -8.7779e-02,  2.4981e-01,  3.5145e-01,\n",
       "          -8.5011e-03, -2.4622e-02, -1.3932e-02, -1.4537e-01, -3.9674e-01],\n",
       "         [ 2.7538e-01, -1.4237e-01, -7.3203e-02,  2.0212e-01,  2.8714e-01,\n",
       "           1.2738e-01,  6.6503e-02, -1.1759e-01, -1.5879e-01, -3.6957e-01],\n",
       "         [ 1.9020e-01, -1.7825e-01, -1.0666e-01,  1.3920e-01,  2.1984e-01,\n",
       "          -9.1898e-03,  1.8954e-03, -6.6220e-03, -2.0956e-01, -3.4140e-01],\n",
       "         [ 2.7479e-01, -2.5092e-01, -3.6905e-02,  3.3468e-01,  4.8446e-01,\n",
       "           6.2289e-02,  2.5815e-03, -4.9874e-03, -1.3632e-01, -5.4192e-01],\n",
       "         [ 2.6442e-01, -1.7209e-01, -9.6289e-02,  2.8380e-01,  3.9823e-01,\n",
       "          -1.8517e-02, -3.4285e-02, -2.7303e-02, -2.0090e-01, -4.0557e-01],\n",
       "         [ 2.1621e-01, -1.6207e-01, -1.6826e-01,  1.8397e-01,  2.3409e-01,\n",
       "           8.8349e-02,  3.9733e-03, -8.8007e-02, -1.3241e-01, -3.3886e-01],\n",
       "         [ 2.9761e-01, -1.9495e-01, -6.3890e-02,  4.2699e-01,  4.6457e-01,\n",
       "           2.8728e-02, -8.9422e-02, -1.3291e-02, -1.6114e-01, -5.8733e-01],\n",
       "         [ 2.6727e-01, -1.9591e-01, -1.3450e-02,  2.5150e-01,  2.7453e-01,\n",
       "          -9.7261e-03,  3.2743e-02, -1.2073e-02, -1.0488e-01, -3.5939e-01],\n",
       "         [ 2.0381e-01, -2.2669e-01, -8.4856e-02,  2.6698e-01,  3.0938e-01,\n",
       "           4.5686e-02, -4.6198e-02, -5.9706e-02, -1.3419e-01, -3.0756e-01],\n",
       "         [ 2.7094e-01, -1.8382e-01, -1.6756e-01,  2.4880e-01,  3.5051e-01,\n",
       "           1.0872e-03, -7.8250e-02, -1.5709e-02, -2.2112e-01, -3.9697e-01],\n",
       "         [ 3.3446e-01, -2.2224e-01, -1.4137e-01,  2.6785e-01,  3.8024e-01,\n",
       "           1.6761e-02,  6.5051e-02, -1.0381e-02, -1.1776e-01, -4.4418e-01],\n",
       "         [ 2.2817e-01, -2.0345e-01, -1.7087e-01,  3.9581e-01,  4.7521e-01,\n",
       "           1.7119e-02, -1.0470e-02, -4.8646e-02, -2.1224e-01, -4.0843e-01],\n",
       "         [ 2.0488e-01, -1.9089e-01, -1.4122e-01,  7.6190e-02,  1.6956e-01,\n",
       "           1.4229e-01,  5.4283e-02, -8.8211e-04, -1.3225e-01, -2.2678e-01],\n",
       "         [ 2.7698e-01, -2.1218e-01, -3.0382e-02,  3.0992e-01,  4.3236e-01,\n",
       "           4.8775e-02, -2.7956e-02, -1.6039e-02, -1.5910e-01, -4.8950e-01],\n",
       "         [ 2.7204e-01, -2.1019e-01, -1.1965e-01,  1.5314e-01,  2.2120e-01,\n",
       "           7.9809e-02,  9.5116e-02, -9.7960e-02, -9.6141e-02, -3.2803e-01],\n",
       "         [ 2.3919e-01, -1.6175e-01, -1.3538e-01,  2.4016e-01,  2.7774e-01,\n",
       "           1.4825e-01, -5.1985e-02,  3.4606e-02, -1.8758e-01, -3.5969e-01],\n",
       "         [ 3.3299e-01, -1.7421e-01, -8.8904e-03,  1.4780e-01,  2.5838e-01,\n",
       "           8.0083e-02,  1.4116e-01, -6.3627e-02, -2.3679e-01, -4.7288e-01],\n",
       "         [ 2.2694e-01, -1.6257e-01, -7.1014e-02,  1.3036e-01,  1.9542e-01,\n",
       "           1.3504e-01,  4.5276e-02, -1.7321e-02, -1.2185e-01, -2.9140e-01],\n",
       "         [ 2.8830e-01, -2.3434e-01, -1.1446e-01,  1.0708e-01,  2.8964e-01,\n",
       "           3.9751e-02,  4.3037e-02, -2.5281e-02, -2.2065e-01, -3.9752e-01]],\n",
       "        device='mps:0', grad_fn=<LinearBackward0>),\n",
       " tensor([[0.0357, 0.0635, 0.0000,  ..., 0.0000, 0.0745, 0.1339],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0550, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1209],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0677, 0.2531, 0.2314],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1699],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0412, 0.0935, 0.2207]],\n",
       "        device='mps:0', grad_fn=<ReluBackward0>),\n",
       " tensor([[0.0000, 0.0778, 0.0000,  ..., 0.0000, 0.1912, 0.0403],\n",
       "         [0.0000, 0.1591, 0.0000,  ..., 0.0000, 0.1598, 0.0000],\n",
       "         [0.0000, 0.0287, 0.0000,  ..., 0.0000, 0.1243, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0467, 0.0000,  ..., 0.0000, 0.3261, 0.0218],\n",
       "         [0.0000, 0.0332, 0.0000,  ..., 0.0000, 0.1763, 0.0000],\n",
       "         [0.0000, 0.1283, 0.0000,  ..., 0.0000, 0.2018, 0.0300]],\n",
       "        device='mps:0', grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/mnist_relu_1')\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        # input: 28 * 28\n",
    "        # after first conv with kernel size 5: 24 * 24\n",
    "        # after first pool: 12 * 12\n",
    "        # after second conv with kernel size 5: 8 * 8\n",
    "        # after second pool: 4 * 4\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=nn.init.calculate_gain('relu')) \n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight, gain=nn.init.calculate_gain('relu')) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        act1 = F.relu(self.fc1(x))\n",
    "        act2 = F.relu(self.fc2(act1))\n",
    "        x = self.fc3(act2)\n",
    "        return x, act1, act2\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "X = X.to(device)\n",
    "x = X[0].to(device)\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1190c9-afd9-4cc8-b839-e717409938f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d23b29-2043-4840-907e-1691d22cb77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current layer size:  150\n",
      "Current layer size:  6\n",
      "Current layer size:  2400\n",
      "Current layer size:  16\n",
      "Current layer size:  30720\n",
      "Current layer size:  120\n",
      "Current layer size:  10080\n",
      "Current layer size:  84\n",
      "Current layer size:  840\n",
      "Current layer size:  10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44426"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_params = 0\n",
    "for params in model.parameters():\n",
    "    current_layer_size = np.prod(params.shape)\n",
    "    print(\"Current layer size: \", current_layer_size)\n",
    "    total_params += current_layer_size\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e756c8-4ebc-4590-b864-4e7763166ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8a0cf0-5a67-48bc-bad0-aac6950a6212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 120]),\n",
       " tensor([[0.0185, 0.0000, 0.1110,  ..., 0.0000, 0.0468, 0.0737],\n",
       "         [0.0135, 0.0000, 0.1729,  ..., 0.0000, 0.0393, 0.1351],\n",
       "         [0.0040, 0.0255, 0.1510,  ..., 0.0765, 0.0122, 0.0022],\n",
       "         ...,\n",
       "         [0.0114, 0.0423, 0.1322,  ..., 0.0027, 0.0009, 0.1191],\n",
       "         [0.0007, 0.0000, 0.1230,  ..., 0.0000, 0.0000, 0.0672],\n",
       "         [0.0000, 0.0537, 0.0738,  ..., 0.0240, 0.0512, 0.0461]],\n",
       "        device='mps:0', grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, act1, act2 = model(X)\n",
    "act1.shape, act1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "347b3d35-154b-4c55-98cd-aed215de5359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(41.3306, device='mps:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1.square().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc40f675-8988-41b5-be33-dc8dc7e9947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch, writer=None):\n",
    "    running_loss = 0.0\n",
    "    running_sum1 = 0.0\n",
    "    running_sqsum1 = 0.0\n",
    "    running_count1 = 0.0\n",
    "    running_sum2 = 0.0\n",
    "    running_sqsum2 = 0.0\n",
    "    running_count2 = 0.0\n",
    "    grads1 = 0.0\n",
    "    grads_count1 = 0\n",
    "    grads_count2 = 0\n",
    "    grads_count3 = 0\n",
    "    grads2 = 0.0\n",
    "    grads3 = 0.0\n",
    "    grads_sq1 = 0.0\n",
    "    grads_sq2 = 0.0\n",
    "    grads_sq3 = 0.0\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred, act1, act2 = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        running_loss += loss\n",
    "        running_sum1 += act1.sum()\n",
    "        running_sqsum1 += act1.square().sum()\n",
    "        running_sum2 += act2.sum()\n",
    "        running_sqsum2 += act2.square().sum()\n",
    "        running_count1 += act1.numel()\n",
    "        running_count2 += act2.numel()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        grads1 += model.fc1.weight.grad.sum()\n",
    "        grads_count1 += model.fc1.weight.grad.numel()\n",
    "        grads_sq1 += model.fc1.weight.grad.square().sum()\n",
    "        grads2 += model.fc2.weight.grad.sum()\n",
    "        grads_count2 += model.fc2.weight.grad.numel()\n",
    "        grads_sq2 += model.fc2.weight.grad.square().sum()\n",
    "        grads3 += model.fc3.weight.grad.sum()\n",
    "        grads_count3 += model.fc3.weight.grad.numel()\n",
    "        grads_sq3 += model.fc3.weight.grad.square().sum()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            step = epoch * len(dataloader) + batch\n",
    "            if writer != None:\n",
    "                writer.add_scalar('training loss',\n",
    "                                running_loss / 100,\n",
    "                                step)\n",
    "                writer.add_scalar(\"act1_mean\",\n",
    "                                  running_sum1 / running_count1,\n",
    "                                  step)\n",
    "                writer.add_scalar(\"act1_stddev\",\n",
    "                                  sqrt(running_sqsum1 / running_count1 - (running_sum1 / running_count1)**2),\n",
    "                                  step)\n",
    "                writer.add_scalar(\"act2_mean\",\n",
    "                                  running_sum2 / running_count2,\n",
    "                                  step)\n",
    "                writer.add_scalar(\"act2_stddev\",\n",
    "                                  sqrt(running_sqsum2 / running_count2 - (running_sum2 / running_count2)**2),\n",
    "                                  step)\n",
    "                writer.add_scalar(\"grad1_mean\",\n",
    "                                  grads1 / grads_count1,\n",
    "                                  step)\n",
    "                writer.add_scalar(\"grad1_stddev\",\n",
    "                                  sqrt(grads_sq1 / grads_count1 - (grads1/grads_count1)**2),\n",
    "                                  step)\n",
    "                writer.add_scalar(\"grad2_mean\",\n",
    "                                  grads2 / grads_count2,\n",
    "                                  step)\n",
    "                writer.add_scalar(\"grad2_stddev\",\n",
    "                                  sqrt(grads_sq2 / grads_count2 - (grads2/grads_count2)**2),\n",
    "                                  step)\n",
    "                writer.add_scalar(\"grad3_mean\",\n",
    "                                  grads3 / grads_count3,\n",
    "                                  step)\n",
    "                writer.add_scalar(\"grad3_stddev\",\n",
    "                                  sqrt(grads_sq3 / grads_count3 - (grads3/grads_count3)**2),\n",
    "                                  step)\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            running_loss = 0.0\n",
    "            running_sum1 = 0.0\n",
    "            running_sqsum1 = 0.0\n",
    "            running_count1 = 0.0\n",
    "            running_sum2 = 0.0\n",
    "            running_sqsum2 = 0.0\n",
    "            running_count2 = 0.0\n",
    "            grads1 = 0.0\n",
    "            grads2 = 0.0\n",
    "            grads3 = 0.0\n",
    "            grads_sq1 = 0.0\n",
    "            grads_sq2 = 0.0\n",
    "            grads_sq3 = 0.0\n",
    "            grads_count1 = 0\n",
    "            grads_count2 = 0\n",
    "            grads_count3 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d982f3-ebce-4b55-9c1c-ac8d70a5e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, epoch, writer=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred, _, _ = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    step = epoch * len(test_dataloader)\n",
    "    if writer != None:\n",
    "        writer.add_scalar('test loss',\n",
    "                            test_loss,\n",
    "                            step)\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0949da63-3d43-44c8-b6bb-3f980fa4e7c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 11.2%, Avg loss: 2.303951 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296570  [   64/60000]\n",
      "loss: 2.304134  [ 6464/60000]\n",
      "loss: 2.305585  [12864/60000]\n",
      "loss: 2.296534  [19264/60000]\n",
      "loss: 2.291893  [25664/60000]\n",
      "loss: 2.306139  [32064/60000]\n",
      "loss: 2.307861  [38464/60000]\n",
      "loss: 2.311530  [44864/60000]\n",
      "loss: 2.303713  [51264/60000]\n",
      "loss: 2.296040  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 16.4%, Avg loss: 2.299900 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.293348  [   64/60000]\n",
      "loss: 2.300620  [ 6464/60000]\n",
      "loss: 2.301673  [12864/60000]\n",
      "loss: 2.293946  [19264/60000]\n",
      "loss: 2.289613  [25664/60000]\n",
      "loss: 2.302775  [32064/60000]\n",
      "loss: 2.301996  [38464/60000]\n",
      "loss: 2.307940  [44864/60000]\n",
      "loss: 2.300944  [51264/60000]\n",
      "loss: 2.292131  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 2.296220 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.290426  [   64/60000]\n",
      "loss: 2.297020  [ 6464/60000]\n",
      "loss: 2.297733  [12864/60000]\n",
      "loss: 2.291466  [19264/60000]\n",
      "loss: 2.287823  [25664/60000]\n",
      "loss: 2.299627  [32064/60000]\n",
      "loss: 2.296196  [38464/60000]\n",
      "loss: 2.304109  [44864/60000]\n",
      "loss: 2.297435  [51264/60000]\n",
      "loss: 2.287807  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.3%, Avg loss: 2.292209 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.287365  [   64/60000]\n",
      "loss: 2.293027  [ 6464/60000]\n",
      "loss: 2.293025  [12864/60000]\n",
      "loss: 2.288525  [19264/60000]\n",
      "loss: 2.285270  [25664/60000]\n",
      "loss: 2.295990  [32064/60000]\n",
      "loss: 2.289350  [38464/60000]\n",
      "loss: 2.299387  [44864/60000]\n",
      "loss: 2.293177  [51264/60000]\n",
      "loss: 2.282203  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 2.287061 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.283409  [   64/60000]\n",
      "loss: 2.287847  [ 6464/60000]\n",
      "loss: 2.286981  [12864/60000]\n",
      "loss: 2.284175  [19264/60000]\n",
      "loss: 2.280670  [25664/60000]\n",
      "loss: 2.290603  [32064/60000]\n",
      "loss: 2.280492  [38464/60000]\n",
      "loss: 2.293202  [44864/60000]\n",
      "loss: 2.287204  [51264/60000]\n",
      "loss: 2.274163  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.2%, Avg loss: 2.279423 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.277085  [   64/60000]\n",
      "loss: 2.280389  [ 6464/60000]\n",
      "loss: 2.277904  [12864/60000]\n",
      "loss: 2.276942  [19264/60000]\n",
      "loss: 2.271856  [25664/60000]\n",
      "loss: 2.280906  [32064/60000]\n",
      "loss: 2.266737  [38464/60000]\n",
      "loss: 2.283063  [44864/60000]\n",
      "loss: 2.276872  [51264/60000]\n",
      "loss: 2.260294  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.4%, Avg loss: 2.265888 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.265776  [   64/60000]\n",
      "loss: 2.266933  [ 6464/60000]\n",
      "loss: 2.261794  [12864/60000]\n",
      "loss: 2.262441  [19264/60000]\n",
      "loss: 2.253672  [25664/60000]\n",
      "loss: 2.261384  [32064/60000]\n",
      "loss: 2.240406  [38464/60000]\n",
      "loss: 2.263623  [44864/60000]\n",
      "loss: 2.254916  [51264/60000]\n",
      "loss: 2.230915  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 2.237182 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.241296  [   64/60000]\n",
      "loss: 2.236387  [ 6464/60000]\n",
      "loss: 2.226199  [12864/60000]\n",
      "loss: 2.228195  [19264/60000]\n",
      "loss: 2.210279  [25664/60000]\n",
      "loss: 2.213816  [32064/60000]\n",
      "loss: 2.175634  [38464/60000]\n",
      "loss: 2.213683  [44864/60000]\n",
      "loss: 2.194778  [51264/60000]\n",
      "loss: 2.150964  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 2.157348 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.173481  [   64/60000]\n",
      "loss: 2.144980  [ 6464/60000]\n",
      "loss: 2.124296  [12864/60000]\n",
      "loss: 2.119051  [19264/60000]\n",
      "loss: 2.070004  [25664/60000]\n",
      "loss: 2.049357  [32064/60000]\n",
      "loss: 1.953368  [38464/60000]\n",
      "loss: 2.032273  [44864/60000]\n",
      "loss: 1.966204  [51264/60000]\n",
      "loss: 1.840467  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.830734 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.876623  [   64/60000]\n",
      "loss: 1.726377  [ 6464/60000]\n",
      "loss: 1.703949  [12864/60000]\n",
      "loss: 1.600567  [19264/60000]\n",
      "loss: 1.472454  [25664/60000]\n",
      "loss: 1.391720  [32064/60000]\n",
      "loss: 1.158702  [38464/60000]\n",
      "loss: 1.359519  [44864/60000]\n",
      "loss: 1.217069  [51264/60000]\n",
      "loss: 1.027407  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.000468 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.087549  [   64/60000]\n",
      "loss: 0.837494  [ 6464/60000]\n",
      "loss: 0.856190  [12864/60000]\n",
      "loss: 0.798977  [19264/60000]\n",
      "loss: 0.822682  [25664/60000]\n",
      "loss: 0.758834  [32064/60000]\n",
      "loss: 0.633127  [38464/60000]\n",
      "loss: 0.843890  [44864/60000]\n",
      "loss: 0.796042  [51264/60000]\n",
      "loss: 0.726178  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.653532 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.760546  [   64/60000]\n",
      "loss: 0.508450  [ 6464/60000]\n",
      "loss: 0.528069  [12864/60000]\n",
      "loss: 0.579205  [19264/60000]\n",
      "loss: 0.664155  [25664/60000]\n",
      "loss: 0.584459  [32064/60000]\n",
      "loss: 0.499457  [38464/60000]\n",
      "loss: 0.675747  [44864/60000]\n",
      "loss: 0.659965  [51264/60000]\n",
      "loss: 0.641220  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.543426 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.678797  [   64/60000]\n",
      "loss: 0.413168  [ 6464/60000]\n",
      "loss: 0.426655  [12864/60000]\n",
      "loss: 0.513288  [19264/60000]\n",
      "loss: 0.597031  [25664/60000]\n",
      "loss: 0.521878  [32064/60000]\n",
      "loss: 0.434887  [38464/60000]\n",
      "loss: 0.594371  [44864/60000]\n",
      "loss: 0.588386  [51264/60000]\n",
      "loss: 0.589560  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.484009 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.629888  [   64/60000]\n",
      "loss: 0.371587  [ 6464/60000]\n",
      "loss: 0.374380  [12864/60000]\n",
      "loss: 0.473573  [19264/60000]\n",
      "loss: 0.550627  [25664/60000]\n",
      "loss: 0.480008  [32064/60000]\n",
      "loss: 0.386060  [38464/60000]\n",
      "loss: 0.542376  [44864/60000]\n",
      "loss: 0.538877  [51264/60000]\n",
      "loss: 0.547938  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.442095 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.587854  [   64/60000]\n",
      "loss: 0.346212  [ 6464/60000]\n",
      "loss: 0.339186  [12864/60000]\n",
      "loss: 0.441770  [19264/60000]\n",
      "loss: 0.508338  [25664/60000]\n",
      "loss: 0.447409  [32064/60000]\n",
      "loss: 0.347552  [38464/60000]\n",
      "loss: 0.506635  [44864/60000]\n",
      "loss: 0.498075  [51264/60000]\n",
      "loss: 0.512328  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.409178 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.550423  [   64/60000]\n",
      "loss: 0.327289  [ 6464/60000]\n",
      "loss: 0.309319  [12864/60000]\n",
      "loss: 0.415198  [19264/60000]\n",
      "loss: 0.468019  [25664/60000]\n",
      "loss: 0.421791  [32064/60000]\n",
      "loss: 0.316726  [38464/60000]\n",
      "loss: 0.480415  [44864/60000]\n",
      "loss: 0.459949  [51264/60000]\n",
      "loss: 0.483400  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.381891 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.516394  [   64/60000]\n",
      "loss: 0.311918  [ 6464/60000]\n",
      "loss: 0.283474  [12864/60000]\n",
      "loss: 0.391543  [19264/60000]\n",
      "loss: 0.430977  [25664/60000]\n",
      "loss: 0.401379  [32064/60000]\n",
      "loss: 0.291216  [38464/60000]\n",
      "loss: 0.460300  [44864/60000]\n",
      "loss: 0.425464  [51264/60000]\n",
      "loss: 0.457799  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.358429 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.484331  [   64/60000]\n",
      "loss: 0.298479  [ 6464/60000]\n",
      "loss: 0.259296  [12864/60000]\n",
      "loss: 0.370264  [19264/60000]\n",
      "loss: 0.396741  [25664/60000]\n",
      "loss: 0.383603  [32064/60000]\n",
      "loss: 0.269983  [38464/60000]\n",
      "loss: 0.443192  [44864/60000]\n",
      "loss: 0.395806  [51264/60000]\n",
      "loss: 0.435092  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.337969 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.455361  [   64/60000]\n",
      "loss: 0.288714  [ 6464/60000]\n",
      "loss: 0.238979  [12864/60000]\n",
      "loss: 0.352830  [19264/60000]\n",
      "loss: 0.365470  [25664/60000]\n",
      "loss: 0.368529  [32064/60000]\n",
      "loss: 0.249710  [38464/60000]\n",
      "loss: 0.427317  [44864/60000]\n",
      "loss: 0.368976  [51264/60000]\n",
      "loss: 0.414276  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.319876 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.428033  [   64/60000]\n",
      "loss: 0.281934  [ 6464/60000]\n",
      "loss: 0.220826  [12864/60000]\n",
      "loss: 0.338520  [19264/60000]\n",
      "loss: 0.337021  [25664/60000]\n",
      "loss: 0.354137  [32064/60000]\n",
      "loss: 0.232721  [38464/60000]\n",
      "loss: 0.414549  [44864/60000]\n",
      "loss: 0.344160  [51264/60000]\n",
      "loss: 0.394453  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.303485 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.403114  [   64/60000]\n",
      "loss: 0.276026  [ 6464/60000]\n",
      "loss: 0.205023  [12864/60000]\n",
      "loss: 0.327147  [19264/60000]\n",
      "loss: 0.311207  [25664/60000]\n",
      "loss: 0.341471  [32064/60000]\n",
      "loss: 0.216495  [38464/60000]\n",
      "loss: 0.403126  [44864/60000]\n",
      "loss: 0.323183  [51264/60000]\n",
      "loss: 0.375759  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.288906 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.380481  [   64/60000]\n",
      "loss: 0.270604  [ 6464/60000]\n",
      "loss: 0.191309  [12864/60000]\n",
      "loss: 0.318143  [19264/60000]\n",
      "loss: 0.288909  [25664/60000]\n",
      "loss: 0.329258  [32064/60000]\n",
      "loss: 0.201704  [38464/60000]\n",
      "loss: 0.393734  [44864/60000]\n",
      "loss: 0.304304  [51264/60000]\n",
      "loss: 0.358914  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.275630 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.359363  [   64/60000]\n",
      "loss: 0.265746  [ 6464/60000]\n",
      "loss: 0.178741  [12864/60000]\n",
      "loss: 0.311817  [19264/60000]\n",
      "loss: 0.269204  [25664/60000]\n",
      "loss: 0.318019  [32064/60000]\n",
      "loss: 0.188520  [38464/60000]\n",
      "loss: 0.385422  [44864/60000]\n",
      "loss: 0.286254  [51264/60000]\n",
      "loss: 0.342744  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.263445 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.339041  [   64/60000]\n",
      "loss: 0.260769  [ 6464/60000]\n",
      "loss: 0.167474  [12864/60000]\n",
      "loss: 0.307457  [19264/60000]\n",
      "loss: 0.250684  [25664/60000]\n",
      "loss: 0.308947  [32064/60000]\n",
      "loss: 0.176602  [38464/60000]\n",
      "loss: 0.376239  [44864/60000]\n",
      "loss: 0.271304  [51264/60000]\n",
      "loss: 0.327241  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.252150 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.319755  [   64/60000]\n",
      "loss: 0.255712  [ 6464/60000]\n",
      "loss: 0.157559  [12864/60000]\n",
      "loss: 0.303931  [19264/60000]\n",
      "loss: 0.233715  [25664/60000]\n",
      "loss: 0.300889  [32064/60000]\n",
      "loss: 0.166930  [38464/60000]\n",
      "loss: 0.367697  [44864/60000]\n",
      "loss: 0.258847  [51264/60000]\n",
      "loss: 0.311569  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.241445 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.302074  [   64/60000]\n",
      "loss: 0.250588  [ 6464/60000]\n",
      "loss: 0.147683  [12864/60000]\n",
      "loss: 0.299024  [19264/60000]\n",
      "loss: 0.217196  [25664/60000]\n",
      "loss: 0.293361  [32064/60000]\n",
      "loss: 0.158009  [38464/60000]\n",
      "loss: 0.358829  [44864/60000]\n",
      "loss: 0.248646  [51264/60000]\n",
      "loss: 0.298260  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.231494 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.285267  [   64/60000]\n",
      "loss: 0.246387  [ 6464/60000]\n",
      "loss: 0.140339  [12864/60000]\n",
      "loss: 0.291655  [19264/60000]\n",
      "loss: 0.202403  [25664/60000]\n",
      "loss: 0.286342  [32064/60000]\n",
      "loss: 0.150058  [38464/60000]\n",
      "loss: 0.351408  [44864/60000]\n",
      "loss: 0.239522  [51264/60000]\n",
      "loss: 0.285604  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.222077 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.268142  [   64/60000]\n",
      "loss: 0.242615  [ 6464/60000]\n",
      "loss: 0.135215  [12864/60000]\n",
      "loss: 0.284540  [19264/60000]\n",
      "loss: 0.187566  [25664/60000]\n",
      "loss: 0.277753  [32064/60000]\n",
      "loss: 0.142401  [38464/60000]\n",
      "loss: 0.344918  [44864/60000]\n",
      "loss: 0.232404  [51264/60000]\n",
      "loss: 0.273867  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.213172 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.252803  [   64/60000]\n",
      "loss: 0.239291  [ 6464/60000]\n",
      "loss: 0.130352  [12864/60000]\n",
      "loss: 0.278454  [19264/60000]\n",
      "loss: 0.174184  [25664/60000]\n",
      "loss: 0.269728  [32064/60000]\n",
      "loss: 0.134839  [38464/60000]\n",
      "loss: 0.339708  [44864/60000]\n",
      "loss: 0.225889  [51264/60000]\n",
      "loss: 0.263483  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.204663 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.237691  [   64/60000]\n",
      "loss: 0.235862  [ 6464/60000]\n",
      "loss: 0.126515  [12864/60000]\n",
      "loss: 0.272574  [19264/60000]\n",
      "loss: 0.161901  [25664/60000]\n",
      "loss: 0.261381  [32064/60000]\n",
      "loss: 0.128030  [38464/60000]\n",
      "loss: 0.333718  [44864/60000]\n",
      "loss: 0.221475  [51264/60000]\n",
      "loss: 0.254639  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.196598 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.223001  [   64/60000]\n",
      "loss: 0.232370  [ 6464/60000]\n",
      "loss: 0.122319  [12864/60000]\n",
      "loss: 0.267074  [19264/60000]\n",
      "loss: 0.150443  [25664/60000]\n",
      "loss: 0.252970  [32064/60000]\n",
      "loss: 0.121805  [38464/60000]\n",
      "loss: 0.327214  [44864/60000]\n",
      "loss: 0.217341  [51264/60000]\n",
      "loss: 0.248758  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.189058 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.208744  [   64/60000]\n",
      "loss: 0.228802  [ 6464/60000]\n",
      "loss: 0.118251  [12864/60000]\n",
      "loss: 0.261066  [19264/60000]\n",
      "loss: 0.140260  [25664/60000]\n",
      "loss: 0.246158  [32064/60000]\n",
      "loss: 0.116021  [38464/60000]\n",
      "loss: 0.321252  [44864/60000]\n",
      "loss: 0.214794  [51264/60000]\n",
      "loss: 0.243558  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.182056 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.195414  [   64/60000]\n",
      "loss: 0.225948  [ 6464/60000]\n",
      "loss: 0.114580  [12864/60000]\n",
      "loss: 0.250823  [19264/60000]\n",
      "loss: 0.131349  [25664/60000]\n",
      "loss: 0.238125  [32064/60000]\n",
      "loss: 0.110168  [38464/60000]\n",
      "loss: 0.315235  [44864/60000]\n",
      "loss: 0.213255  [51264/60000]\n",
      "loss: 0.239284  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.175384 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.182931  [   64/60000]\n",
      "loss: 0.222488  [ 6464/60000]\n",
      "loss: 0.111231  [12864/60000]\n",
      "loss: 0.242270  [19264/60000]\n",
      "loss: 0.122140  [25664/60000]\n",
      "loss: 0.231739  [32064/60000]\n",
      "loss: 0.105100  [38464/60000]\n",
      "loss: 0.308504  [44864/60000]\n",
      "loss: 0.211018  [51264/60000]\n",
      "loss: 0.235444  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.169043 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.170587  [   64/60000]\n",
      "loss: 0.218774  [ 6464/60000]\n",
      "loss: 0.108093  [12864/60000]\n",
      "loss: 0.235362  [19264/60000]\n",
      "loss: 0.113975  [25664/60000]\n",
      "loss: 0.225834  [32064/60000]\n",
      "loss: 0.099637  [38464/60000]\n",
      "loss: 0.301848  [44864/60000]\n",
      "loss: 0.208872  [51264/60000]\n",
      "loss: 0.231932  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.163128 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.160336  [   64/60000]\n",
      "loss: 0.214207  [ 6464/60000]\n",
      "loss: 0.106695  [12864/60000]\n",
      "loss: 0.228515  [19264/60000]\n",
      "loss: 0.106636  [25664/60000]\n",
      "loss: 0.220282  [32064/60000]\n",
      "loss: 0.094770  [38464/60000]\n",
      "loss: 0.295295  [44864/60000]\n",
      "loss: 0.207175  [51264/60000]\n",
      "loss: 0.228092  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.157560 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.150765  [   64/60000]\n",
      "loss: 0.211138  [ 6464/60000]\n",
      "loss: 0.104008  [12864/60000]\n",
      "loss: 0.222313  [19264/60000]\n",
      "loss: 0.100529  [25664/60000]\n",
      "loss: 0.214737  [32064/60000]\n",
      "loss: 0.090592  [38464/60000]\n",
      "loss: 0.289479  [44864/60000]\n",
      "loss: 0.205184  [51264/60000]\n",
      "loss: 0.224515  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.152370 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.141629  [   64/60000]\n",
      "loss: 0.207475  [ 6464/60000]\n",
      "loss: 0.101944  [12864/60000]\n",
      "loss: 0.217102  [19264/60000]\n",
      "loss: 0.095026  [25664/60000]\n",
      "loss: 0.208020  [32064/60000]\n",
      "loss: 0.086785  [38464/60000]\n",
      "loss: 0.282564  [44864/60000]\n",
      "loss: 0.204449  [51264/60000]\n",
      "loss: 0.221130  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.147441 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.133570  [   64/60000]\n",
      "loss: 0.204097  [ 6464/60000]\n",
      "loss: 0.100379  [12864/60000]\n",
      "loss: 0.210833  [19264/60000]\n",
      "loss: 0.089268  [25664/60000]\n",
      "loss: 0.202462  [32064/60000]\n",
      "loss: 0.083533  [38464/60000]\n",
      "loss: 0.276845  [44864/60000]\n",
      "loss: 0.202791  [51264/60000]\n",
      "loss: 0.217786  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.7%, Avg loss: 0.142885 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.125648  [   64/60000]\n",
      "loss: 0.200448  [ 6464/60000]\n",
      "loss: 0.098776  [12864/60000]\n",
      "loss: 0.204909  [19264/60000]\n",
      "loss: 0.084471  [25664/60000]\n",
      "loss: 0.197718  [32064/60000]\n",
      "loss: 0.081228  [38464/60000]\n",
      "loss: 0.270967  [44864/60000]\n",
      "loss: 0.201548  [51264/60000]\n",
      "loss: 0.214717  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.138560 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.117647  [   64/60000]\n",
      "loss: 0.197072  [ 6464/60000]\n",
      "loss: 0.096819  [12864/60000]\n",
      "loss: 0.199369  [19264/60000]\n",
      "loss: 0.079912  [25664/60000]\n",
      "loss: 0.193495  [32064/60000]\n",
      "loss: 0.078909  [38464/60000]\n",
      "loss: 0.264347  [44864/60000]\n",
      "loss: 0.200592  [51264/60000]\n",
      "loss: 0.211708  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.134559 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.110455  [   64/60000]\n",
      "loss: 0.193634  [ 6464/60000]\n",
      "loss: 0.095021  [12864/60000]\n",
      "loss: 0.194995  [19264/60000]\n",
      "loss: 0.075483  [25664/60000]\n",
      "loss: 0.189610  [32064/60000]\n",
      "loss: 0.077284  [38464/60000]\n",
      "loss: 0.258461  [44864/60000]\n",
      "loss: 0.199414  [51264/60000]\n",
      "loss: 0.209374  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.130824 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.104339  [   64/60000]\n",
      "loss: 0.190581  [ 6464/60000]\n",
      "loss: 0.093421  [12864/60000]\n",
      "loss: 0.191084  [19264/60000]\n",
      "loss: 0.072150  [25664/60000]\n",
      "loss: 0.185677  [32064/60000]\n",
      "loss: 0.075856  [38464/60000]\n",
      "loss: 0.252718  [44864/60000]\n",
      "loss: 0.197877  [51264/60000]\n",
      "loss: 0.206869  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 0.127280 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.098534  [   64/60000]\n",
      "loss: 0.188526  [ 6464/60000]\n",
      "loss: 0.092488  [12864/60000]\n",
      "loss: 0.186498  [19264/60000]\n",
      "loss: 0.069009  [25664/60000]\n",
      "loss: 0.181803  [32064/60000]\n",
      "loss: 0.074414  [38464/60000]\n",
      "loss: 0.246540  [44864/60000]\n",
      "loss: 0.196582  [51264/60000]\n",
      "loss: 0.204286  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.124060 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.093309  [   64/60000]\n",
      "loss: 0.185928  [ 6464/60000]\n",
      "loss: 0.091091  [12864/60000]\n",
      "loss: 0.181839  [19264/60000]\n",
      "loss: 0.066190  [25664/60000]\n",
      "loss: 0.178400  [32064/60000]\n",
      "loss: 0.073601  [38464/60000]\n",
      "loss: 0.240449  [44864/60000]\n",
      "loss: 0.194110  [51264/60000]\n",
      "loss: 0.202492  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.120956 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.088635  [   64/60000]\n",
      "loss: 0.183816  [ 6464/60000]\n",
      "loss: 0.089641  [12864/60000]\n",
      "loss: 0.178601  [19264/60000]\n",
      "loss: 0.063509  [25664/60000]\n",
      "loss: 0.175307  [32064/60000]\n",
      "loss: 0.073094  [38464/60000]\n",
      "loss: 0.234954  [44864/60000]\n",
      "loss: 0.191776  [51264/60000]\n",
      "loss: 0.200458  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.118128 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.084327  [   64/60000]\n",
      "loss: 0.180933  [ 6464/60000]\n",
      "loss: 0.088915  [12864/60000]\n",
      "loss: 0.174926  [19264/60000]\n",
      "loss: 0.061266  [25664/60000]\n",
      "loss: 0.172598  [32064/60000]\n",
      "loss: 0.072714  [38464/60000]\n",
      "loss: 0.229302  [44864/60000]\n",
      "loss: 0.190135  [51264/60000]\n",
      "loss: 0.198668  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.115447 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.080367  [   64/60000]\n",
      "loss: 0.179128  [ 6464/60000]\n",
      "loss: 0.088004  [12864/60000]\n",
      "loss: 0.170443  [19264/60000]\n",
      "loss: 0.058778  [25664/60000]\n",
      "loss: 0.169781  [32064/60000]\n",
      "loss: 0.072460  [38464/60000]\n",
      "loss: 0.223580  [44864/60000]\n",
      "loss: 0.187845  [51264/60000]\n",
      "loss: 0.196904  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.112824 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.077091  [   64/60000]\n",
      "loss: 0.176576  [ 6464/60000]\n",
      "loss: 0.087200  [12864/60000]\n",
      "loss: 0.165697  [19264/60000]\n",
      "loss: 0.056628  [25664/60000]\n",
      "loss: 0.167064  [32064/60000]\n",
      "loss: 0.071918  [38464/60000]\n",
      "loss: 0.218952  [44864/60000]\n",
      "loss: 0.186415  [51264/60000]\n",
      "loss: 0.194666  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.110451 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.073658  [   64/60000]\n",
      "loss: 0.174687  [ 6464/60000]\n",
      "loss: 0.086383  [12864/60000]\n",
      "loss: 0.161283  [19264/60000]\n",
      "loss: 0.054280  [25664/60000]\n",
      "loss: 0.164540  [32064/60000]\n",
      "loss: 0.071579  [38464/60000]\n",
      "loss: 0.213889  [44864/60000]\n",
      "loss: 0.185143  [51264/60000]\n",
      "loss: 0.192971  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.108194 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.070498  [   64/60000]\n",
      "loss: 0.172930  [ 6464/60000]\n",
      "loss: 0.085710  [12864/60000]\n",
      "loss: 0.156695  [19264/60000]\n",
      "loss: 0.052771  [25664/60000]\n",
      "loss: 0.161096  [32064/60000]\n",
      "loss: 0.071311  [38464/60000]\n",
      "loss: 0.208979  [44864/60000]\n",
      "loss: 0.183116  [51264/60000]\n",
      "loss: 0.191101  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.106072 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.067554  [   64/60000]\n",
      "loss: 0.171204  [ 6464/60000]\n",
      "loss: 0.085559  [12864/60000]\n",
      "loss: 0.153108  [19264/60000]\n",
      "loss: 0.051096  [25664/60000]\n",
      "loss: 0.158343  [32064/60000]\n",
      "loss: 0.070838  [38464/60000]\n",
      "loss: 0.204544  [44864/60000]\n",
      "loss: 0.180771  [51264/60000]\n",
      "loss: 0.189365  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.104122 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.065001  [   64/60000]\n",
      "loss: 0.169676  [ 6464/60000]\n",
      "loss: 0.085530  [12864/60000]\n",
      "loss: 0.148630  [19264/60000]\n",
      "loss: 0.049568  [25664/60000]\n",
      "loss: 0.155752  [32064/60000]\n",
      "loss: 0.070853  [38464/60000]\n",
      "loss: 0.199694  [44864/60000]\n",
      "loss: 0.179568  [51264/60000]\n",
      "loss: 0.187710  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.102139 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.062622  [   64/60000]\n",
      "loss: 0.168161  [ 6464/60000]\n",
      "loss: 0.085662  [12864/60000]\n",
      "loss: 0.144566  [19264/60000]\n",
      "loss: 0.047914  [25664/60000]\n",
      "loss: 0.153052  [32064/60000]\n",
      "loss: 0.070973  [38464/60000]\n",
      "loss: 0.194816  [44864/60000]\n",
      "loss: 0.177414  [51264/60000]\n",
      "loss: 0.185722  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.100404 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.060506  [   64/60000]\n",
      "loss: 0.166703  [ 6464/60000]\n",
      "loss: 0.085427  [12864/60000]\n",
      "loss: 0.141779  [19264/60000]\n",
      "loss: 0.046248  [25664/60000]\n",
      "loss: 0.150628  [32064/60000]\n",
      "loss: 0.070927  [38464/60000]\n",
      "loss: 0.190347  [44864/60000]\n",
      "loss: 0.175863  [51264/60000]\n",
      "loss: 0.184281  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.098786 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.058258  [   64/60000]\n",
      "loss: 0.164691  [ 6464/60000]\n",
      "loss: 0.085481  [12864/60000]\n",
      "loss: 0.138361  [19264/60000]\n",
      "loss: 0.044889  [25664/60000]\n",
      "loss: 0.147968  [32064/60000]\n",
      "loss: 0.071002  [38464/60000]\n",
      "loss: 0.186045  [44864/60000]\n",
      "loss: 0.174855  [51264/60000]\n",
      "loss: 0.182828  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.097271 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.056754  [   64/60000]\n",
      "loss: 0.162667  [ 6464/60000]\n",
      "loss: 0.085342  [12864/60000]\n",
      "loss: 0.136006  [19264/60000]\n",
      "loss: 0.043153  [25664/60000]\n",
      "loss: 0.145256  [32064/60000]\n",
      "loss: 0.070732  [38464/60000]\n",
      "loss: 0.182289  [44864/60000]\n",
      "loss: 0.173273  [51264/60000]\n",
      "loss: 0.181589  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.095819 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.054836  [   64/60000]\n",
      "loss: 0.160126  [ 6464/60000]\n",
      "loss: 0.085187  [12864/60000]\n",
      "loss: 0.132866  [19264/60000]\n",
      "loss: 0.041605  [25664/60000]\n",
      "loss: 0.143329  [32064/60000]\n",
      "loss: 0.069963  [38464/60000]\n",
      "loss: 0.178064  [44864/60000]\n",
      "loss: 0.171669  [51264/60000]\n",
      "loss: 0.180315  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.094363 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.052958  [   64/60000]\n",
      "loss: 0.158211  [ 6464/60000]\n",
      "loss: 0.085326  [12864/60000]\n",
      "loss: 0.129272  [19264/60000]\n",
      "loss: 0.040332  [25664/60000]\n",
      "loss: 0.141457  [32064/60000]\n",
      "loss: 0.069908  [38464/60000]\n",
      "loss: 0.174297  [44864/60000]\n",
      "loss: 0.169615  [51264/60000]\n",
      "loss: 0.178353  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.093012 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.051427  [   64/60000]\n",
      "loss: 0.156172  [ 6464/60000]\n",
      "loss: 0.085513  [12864/60000]\n",
      "loss: 0.126101  [19264/60000]\n",
      "loss: 0.039219  [25664/60000]\n",
      "loss: 0.139199  [32064/60000]\n",
      "loss: 0.069555  [38464/60000]\n",
      "loss: 0.171051  [44864/60000]\n",
      "loss: 0.168497  [51264/60000]\n",
      "loss: 0.176418  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.091683 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.049963  [   64/60000]\n",
      "loss: 0.153954  [ 6464/60000]\n",
      "loss: 0.085441  [12864/60000]\n",
      "loss: 0.123089  [19264/60000]\n",
      "loss: 0.037876  [25664/60000]\n",
      "loss: 0.137514  [32064/60000]\n",
      "loss: 0.069146  [38464/60000]\n",
      "loss: 0.167729  [44864/60000]\n",
      "loss: 0.166683  [51264/60000]\n",
      "loss: 0.174435  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.090495 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.048458  [   64/60000]\n",
      "loss: 0.151613  [ 6464/60000]\n",
      "loss: 0.085600  [12864/60000]\n",
      "loss: 0.120679  [19264/60000]\n",
      "loss: 0.036603  [25664/60000]\n",
      "loss: 0.135316  [32064/60000]\n",
      "loss: 0.068610  [38464/60000]\n",
      "loss: 0.164708  [44864/60000]\n",
      "loss: 0.164899  [51264/60000]\n",
      "loss: 0.172858  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.089311 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.047106  [   64/60000]\n",
      "loss: 0.149685  [ 6464/60000]\n",
      "loss: 0.085324  [12864/60000]\n",
      "loss: 0.118185  [19264/60000]\n",
      "loss: 0.035603  [25664/60000]\n",
      "loss: 0.133136  [32064/60000]\n",
      "loss: 0.068509  [38464/60000]\n",
      "loss: 0.161455  [44864/60000]\n",
      "loss: 0.162461  [51264/60000]\n",
      "loss: 0.171231  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.088170 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.045583  [   64/60000]\n",
      "loss: 0.148554  [ 6464/60000]\n",
      "loss: 0.085382  [12864/60000]\n",
      "loss: 0.115210  [19264/60000]\n",
      "loss: 0.034458  [25664/60000]\n",
      "loss: 0.131498  [32064/60000]\n",
      "loss: 0.068079  [38464/60000]\n",
      "loss: 0.158710  [44864/60000]\n",
      "loss: 0.160933  [51264/60000]\n",
      "loss: 0.169928  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.087104 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.044042  [   64/60000]\n",
      "loss: 0.146544  [ 6464/60000]\n",
      "loss: 0.085698  [12864/60000]\n",
      "loss: 0.112536  [19264/60000]\n",
      "loss: 0.033301  [25664/60000]\n",
      "loss: 0.129120  [32064/60000]\n",
      "loss: 0.067977  [38464/60000]\n",
      "loss: 0.156006  [44864/60000]\n",
      "loss: 0.159113  [51264/60000]\n",
      "loss: 0.168675  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.086050 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.042924  [   64/60000]\n",
      "loss: 0.145252  [ 6464/60000]\n",
      "loss: 0.085657  [12864/60000]\n",
      "loss: 0.110980  [19264/60000]\n",
      "loss: 0.032063  [25664/60000]\n",
      "loss: 0.127255  [32064/60000]\n",
      "loss: 0.067696  [38464/60000]\n",
      "loss: 0.153362  [44864/60000]\n",
      "loss: 0.156911  [51264/60000]\n",
      "loss: 0.167082  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.085156 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.041589  [   64/60000]\n",
      "loss: 0.143595  [ 6464/60000]\n",
      "loss: 0.086186  [12864/60000]\n",
      "loss: 0.108445  [19264/60000]\n",
      "loss: 0.031072  [25664/60000]\n",
      "loss: 0.125595  [32064/60000]\n",
      "loss: 0.067398  [38464/60000]\n",
      "loss: 0.150884  [44864/60000]\n",
      "loss: 0.155460  [51264/60000]\n",
      "loss: 0.166159  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.084299 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.040725  [   64/60000]\n",
      "loss: 0.142560  [ 6464/60000]\n",
      "loss: 0.085990  [12864/60000]\n",
      "loss: 0.106300  [19264/60000]\n",
      "loss: 0.029790  [25664/60000]\n",
      "loss: 0.123865  [32064/60000]\n",
      "loss: 0.067030  [38464/60000]\n",
      "loss: 0.148389  [44864/60000]\n",
      "loss: 0.154692  [51264/60000]\n",
      "loss: 0.164752  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.083345 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.039393  [   64/60000]\n",
      "loss: 0.141515  [ 6464/60000]\n",
      "loss: 0.086048  [12864/60000]\n",
      "loss: 0.103858  [19264/60000]\n",
      "loss: 0.029005  [25664/60000]\n",
      "loss: 0.121770  [32064/60000]\n",
      "loss: 0.066756  [38464/60000]\n",
      "loss: 0.146297  [44864/60000]\n",
      "loss: 0.153202  [51264/60000]\n",
      "loss: 0.163757  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.082519 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.038380  [   64/60000]\n",
      "loss: 0.140429  [ 6464/60000]\n",
      "loss: 0.086182  [12864/60000]\n",
      "loss: 0.102568  [19264/60000]\n",
      "loss: 0.028039  [25664/60000]\n",
      "loss: 0.119941  [32064/60000]\n",
      "loss: 0.066262  [38464/60000]\n",
      "loss: 0.144078  [44864/60000]\n",
      "loss: 0.151835  [51264/60000]\n",
      "loss: 0.162150  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.081713 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.037593  [   64/60000]\n",
      "loss: 0.138297  [ 6464/60000]\n",
      "loss: 0.086235  [12864/60000]\n",
      "loss: 0.100149  [19264/60000]\n",
      "loss: 0.027107  [25664/60000]\n",
      "loss: 0.117273  [32064/60000]\n",
      "loss: 0.065582  [38464/60000]\n",
      "loss: 0.142135  [44864/60000]\n",
      "loss: 0.150374  [51264/60000]\n",
      "loss: 0.160897  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.080897 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.036559  [   64/60000]\n",
      "loss: 0.137113  [ 6464/60000]\n",
      "loss: 0.086388  [12864/60000]\n",
      "loss: 0.098133  [19264/60000]\n",
      "loss: 0.026236  [25664/60000]\n",
      "loss: 0.115662  [32064/60000]\n",
      "loss: 0.065151  [38464/60000]\n",
      "loss: 0.140429  [44864/60000]\n",
      "loss: 0.148767  [51264/60000]\n",
      "loss: 0.159783  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.080165 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.035832  [   64/60000]\n",
      "loss: 0.134805  [ 6464/60000]\n",
      "loss: 0.086369  [12864/60000]\n",
      "loss: 0.097255  [19264/60000]\n",
      "loss: 0.025465  [25664/60000]\n",
      "loss: 0.114041  [32064/60000]\n",
      "loss: 0.064802  [38464/60000]\n",
      "loss: 0.138120  [44864/60000]\n",
      "loss: 0.146756  [51264/60000]\n",
      "loss: 0.158438  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.079432 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.034892  [   64/60000]\n",
      "loss: 0.133447  [ 6464/60000]\n",
      "loss: 0.086168  [12864/60000]\n",
      "loss: 0.095538  [19264/60000]\n",
      "loss: 0.024790  [25664/60000]\n",
      "loss: 0.112407  [32064/60000]\n",
      "loss: 0.064317  [38464/60000]\n",
      "loss: 0.136394  [44864/60000]\n",
      "loss: 0.146034  [51264/60000]\n",
      "loss: 0.157282  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.078737 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.033850  [   64/60000]\n",
      "loss: 0.132298  [ 6464/60000]\n",
      "loss: 0.086172  [12864/60000]\n",
      "loss: 0.094766  [19264/60000]\n",
      "loss: 0.024076  [25664/60000]\n",
      "loss: 0.110290  [32064/60000]\n",
      "loss: 0.064264  [38464/60000]\n",
      "loss: 0.134738  [44864/60000]\n",
      "loss: 0.144449  [51264/60000]\n",
      "loss: 0.155688  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.078012 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.033154  [   64/60000]\n",
      "loss: 0.130802  [ 6464/60000]\n",
      "loss: 0.086385  [12864/60000]\n",
      "loss: 0.092816  [19264/60000]\n",
      "loss: 0.023296  [25664/60000]\n",
      "loss: 0.108261  [32064/60000]\n",
      "loss: 0.064131  [38464/60000]\n",
      "loss: 0.133397  [44864/60000]\n",
      "loss: 0.144356  [51264/60000]\n",
      "loss: 0.154812  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.077361 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.032390  [   64/60000]\n",
      "loss: 0.129200  [ 6464/60000]\n",
      "loss: 0.086386  [12864/60000]\n",
      "loss: 0.091417  [19264/60000]\n",
      "loss: 0.022513  [25664/60000]\n",
      "loss: 0.106469  [32064/60000]\n",
      "loss: 0.064013  [38464/60000]\n",
      "loss: 0.131426  [44864/60000]\n",
      "loss: 0.143196  [51264/60000]\n",
      "loss: 0.153892  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.076736 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.031833  [   64/60000]\n",
      "loss: 0.127430  [ 6464/60000]\n",
      "loss: 0.086394  [12864/60000]\n",
      "loss: 0.090134  [19264/60000]\n",
      "loss: 0.021817  [25664/60000]\n",
      "loss: 0.104406  [32064/60000]\n",
      "loss: 0.063766  [38464/60000]\n",
      "loss: 0.129860  [44864/60000]\n",
      "loss: 0.142155  [51264/60000]\n",
      "loss: 0.152844  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.076100 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.030985  [   64/60000]\n",
      "loss: 0.126180  [ 6464/60000]\n",
      "loss: 0.086097  [12864/60000]\n",
      "loss: 0.089078  [19264/60000]\n",
      "loss: 0.021312  [25664/60000]\n",
      "loss: 0.102820  [32064/60000]\n",
      "loss: 0.063546  [38464/60000]\n",
      "loss: 0.128326  [44864/60000]\n",
      "loss: 0.141213  [51264/60000]\n",
      "loss: 0.151632  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.075467 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.030336  [   64/60000]\n",
      "loss: 0.125150  [ 6464/60000]\n",
      "loss: 0.086085  [12864/60000]\n",
      "loss: 0.087496  [19264/60000]\n",
      "loss: 0.020790  [25664/60000]\n",
      "loss: 0.101075  [32064/60000]\n",
      "loss: 0.063318  [38464/60000]\n",
      "loss: 0.126778  [44864/60000]\n",
      "loss: 0.140461  [51264/60000]\n",
      "loss: 0.151014  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.074854 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.029646  [   64/60000]\n",
      "loss: 0.124059  [ 6464/60000]\n",
      "loss: 0.086115  [12864/60000]\n",
      "loss: 0.086913  [19264/60000]\n",
      "loss: 0.020050  [25664/60000]\n",
      "loss: 0.099416  [32064/60000]\n",
      "loss: 0.062842  [38464/60000]\n",
      "loss: 0.125939  [44864/60000]\n",
      "loss: 0.139317  [51264/60000]\n",
      "loss: 0.149908  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.074277 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.029071  [   64/60000]\n",
      "loss: 0.123421  [ 6464/60000]\n",
      "loss: 0.085775  [12864/60000]\n",
      "loss: 0.086164  [19264/60000]\n",
      "loss: 0.019625  [25664/60000]\n",
      "loss: 0.097911  [32064/60000]\n",
      "loss: 0.062549  [38464/60000]\n",
      "loss: 0.124487  [44864/60000]\n",
      "loss: 0.138951  [51264/60000]\n",
      "loss: 0.148490  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.073659 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.028414  [   64/60000]\n",
      "loss: 0.122463  [ 6464/60000]\n",
      "loss: 0.085890  [12864/60000]\n",
      "loss: 0.085386  [19264/60000]\n",
      "loss: 0.019135  [25664/60000]\n",
      "loss: 0.096289  [32064/60000]\n",
      "loss: 0.062231  [38464/60000]\n",
      "loss: 0.123407  [44864/60000]\n",
      "loss: 0.137776  [51264/60000]\n",
      "loss: 0.147535  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.073152 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.027931  [   64/60000]\n",
      "loss: 0.121399  [ 6464/60000]\n",
      "loss: 0.085656  [12864/60000]\n",
      "loss: 0.084801  [19264/60000]\n",
      "loss: 0.018542  [25664/60000]\n",
      "loss: 0.094594  [32064/60000]\n",
      "loss: 0.062008  [38464/60000]\n",
      "loss: 0.122132  [44864/60000]\n",
      "loss: 0.137204  [51264/60000]\n",
      "loss: 0.146557  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.072659 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.027452  [   64/60000]\n",
      "loss: 0.120526  [ 6464/60000]\n",
      "loss: 0.085591  [12864/60000]\n",
      "loss: 0.082469  [19264/60000]\n",
      "loss: 0.018064  [25664/60000]\n",
      "loss: 0.092930  [32064/60000]\n",
      "loss: 0.061447  [38464/60000]\n",
      "loss: 0.121182  [44864/60000]\n",
      "loss: 0.136429  [51264/60000]\n",
      "loss: 0.145600  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.072136 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.027185  [   64/60000]\n",
      "loss: 0.119558  [ 6464/60000]\n",
      "loss: 0.085144  [12864/60000]\n",
      "loss: 0.081348  [19264/60000]\n",
      "loss: 0.017608  [25664/60000]\n",
      "loss: 0.091284  [32064/60000]\n",
      "loss: 0.060940  [38464/60000]\n",
      "loss: 0.120220  [44864/60000]\n",
      "loss: 0.135140  [51264/60000]\n",
      "loss: 0.144447  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.071648 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.026808  [   64/60000]\n",
      "loss: 0.118757  [ 6464/60000]\n",
      "loss: 0.084595  [12864/60000]\n",
      "loss: 0.080785  [19264/60000]\n",
      "loss: 0.017141  [25664/60000]\n",
      "loss: 0.090047  [32064/60000]\n",
      "loss: 0.060659  [38464/60000]\n",
      "loss: 0.119304  [44864/60000]\n",
      "loss: 0.133896  [51264/60000]\n",
      "loss: 0.143335  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.071138 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.026378  [   64/60000]\n",
      "loss: 0.117514  [ 6464/60000]\n",
      "loss: 0.084468  [12864/60000]\n",
      "loss: 0.079518  [19264/60000]\n",
      "loss: 0.016703  [25664/60000]\n",
      "loss: 0.088703  [32064/60000]\n",
      "loss: 0.060390  [38464/60000]\n",
      "loss: 0.118710  [44864/60000]\n",
      "loss: 0.132685  [51264/60000]\n",
      "loss: 0.142386  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.070591 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.025702  [   64/60000]\n",
      "loss: 0.116755  [ 6464/60000]\n",
      "loss: 0.084237  [12864/60000]\n",
      "loss: 0.078596  [19264/60000]\n",
      "loss: 0.016395  [25664/60000]\n",
      "loss: 0.087243  [32064/60000]\n",
      "loss: 0.059909  [38464/60000]\n",
      "loss: 0.117545  [44864/60000]\n",
      "loss: 0.131193  [51264/60000]\n",
      "loss: 0.141471  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.070170 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.025375  [   64/60000]\n",
      "loss: 0.116209  [ 6464/60000]\n",
      "loss: 0.084415  [12864/60000]\n",
      "loss: 0.077631  [19264/60000]\n",
      "loss: 0.015980  [25664/60000]\n",
      "loss: 0.086052  [32064/60000]\n",
      "loss: 0.059853  [38464/60000]\n",
      "loss: 0.117101  [44864/60000]\n",
      "loss: 0.129860  [51264/60000]\n",
      "loss: 0.140518  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.069726 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.024755  [   64/60000]\n",
      "loss: 0.115558  [ 6464/60000]\n",
      "loss: 0.084168  [12864/60000]\n",
      "loss: 0.076364  [19264/60000]\n",
      "loss: 0.015695  [25664/60000]\n",
      "loss: 0.085268  [32064/60000]\n",
      "loss: 0.059399  [38464/60000]\n",
      "loss: 0.116104  [44864/60000]\n",
      "loss: 0.129021  [51264/60000]\n",
      "loss: 0.139426  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.069202 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.024533  [   64/60000]\n",
      "loss: 0.114545  [ 6464/60000]\n",
      "loss: 0.083851  [12864/60000]\n",
      "loss: 0.074717  [19264/60000]\n",
      "loss: 0.015345  [25664/60000]\n",
      "loss: 0.084241  [32064/60000]\n",
      "loss: 0.058913  [38464/60000]\n",
      "loss: 0.115902  [44864/60000]\n",
      "loss: 0.128277  [51264/60000]\n",
      "loss: 0.138895  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.068761 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.024091  [   64/60000]\n",
      "loss: 0.113455  [ 6464/60000]\n",
      "loss: 0.083697  [12864/60000]\n",
      "loss: 0.074577  [19264/60000]\n",
      "loss: 0.015027  [25664/60000]\n",
      "loss: 0.082966  [32064/60000]\n",
      "loss: 0.058136  [38464/60000]\n",
      "loss: 0.115296  [44864/60000]\n",
      "loss: 0.128436  [51264/60000]\n",
      "loss: 0.138439  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.068304 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.023902  [   64/60000]\n",
      "loss: 0.112620  [ 6464/60000]\n",
      "loss: 0.083012  [12864/60000]\n",
      "loss: 0.074526  [19264/60000]\n",
      "loss: 0.014699  [25664/60000]\n",
      "loss: 0.082048  [32064/60000]\n",
      "loss: 0.057559  [38464/60000]\n",
      "loss: 0.115101  [44864/60000]\n",
      "loss: 0.127067  [51264/60000]\n",
      "loss: 0.137601  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.068013 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.024003  [   64/60000]\n",
      "loss: 0.111465  [ 6464/60000]\n",
      "loss: 0.083111  [12864/60000]\n",
      "loss: 0.073690  [19264/60000]\n",
      "loss: 0.014371  [25664/60000]\n",
      "loss: 0.080913  [32064/60000]\n",
      "loss: 0.057157  [38464/60000]\n",
      "loss: 0.114252  [44864/60000]\n",
      "loss: 0.126054  [51264/60000]\n",
      "loss: 0.136539  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.067670 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.023574  [   64/60000]\n",
      "loss: 0.111115  [ 6464/60000]\n",
      "loss: 0.082839  [12864/60000]\n",
      "loss: 0.072724  [19264/60000]\n",
      "loss: 0.014014  [25664/60000]\n",
      "loss: 0.079744  [32064/60000]\n",
      "loss: 0.056708  [38464/60000]\n",
      "loss: 0.113448  [44864/60000]\n",
      "loss: 0.125229  [51264/60000]\n",
      "loss: 0.135721  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.067273 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.023185  [   64/60000]\n",
      "loss: 0.110844  [ 6464/60000]\n",
      "loss: 0.081876  [12864/60000]\n",
      "loss: 0.071612  [19264/60000]\n",
      "loss: 0.013771  [25664/60000]\n",
      "loss: 0.078952  [32064/60000]\n",
      "loss: 0.056716  [38464/60000]\n",
      "loss: 0.112957  [44864/60000]\n",
      "loss: 0.124497  [51264/60000]\n",
      "loss: 0.134770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.066874 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.022863  [   64/60000]\n",
      "loss: 0.109962  [ 6464/60000]\n",
      "loss: 0.081556  [12864/60000]\n",
      "loss: 0.071037  [19264/60000]\n",
      "loss: 0.013549  [25664/60000]\n",
      "loss: 0.077854  [32064/60000]\n",
      "loss: 0.056481  [38464/60000]\n",
      "loss: 0.112357  [44864/60000]\n",
      "loss: 0.123349  [51264/60000]\n",
      "loss: 0.133928  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.066514 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.022523  [   64/60000]\n",
      "loss: 0.109479  [ 6464/60000]\n",
      "loss: 0.080624  [12864/60000]\n",
      "loss: 0.070462  [19264/60000]\n",
      "loss: 0.013202  [25664/60000]\n",
      "loss: 0.076909  [32064/60000]\n",
      "loss: 0.056201  [38464/60000]\n",
      "loss: 0.111916  [44864/60000]\n",
      "loss: 0.122455  [51264/60000]\n",
      "loss: 0.132802  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.066122 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.022195  [   64/60000]\n",
      "loss: 0.108795  [ 6464/60000]\n",
      "loss: 0.080193  [12864/60000]\n",
      "loss: 0.070026  [19264/60000]\n",
      "loss: 0.013157  [25664/60000]\n",
      "loss: 0.076005  [32064/60000]\n",
      "loss: 0.056285  [38464/60000]\n",
      "loss: 0.111301  [44864/60000]\n",
      "loss: 0.121444  [51264/60000]\n",
      "loss: 0.132245  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.065719 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.021820  [   64/60000]\n",
      "loss: 0.108381  [ 6464/60000]\n",
      "loss: 0.079946  [12864/60000]\n",
      "loss: 0.069026  [19264/60000]\n",
      "loss: 0.012821  [25664/60000]\n",
      "loss: 0.075028  [32064/60000]\n",
      "loss: 0.056108  [38464/60000]\n",
      "loss: 0.110687  [44864/60000]\n",
      "loss: 0.120469  [51264/60000]\n",
      "loss: 0.131459  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.065371 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.021472  [   64/60000]\n",
      "loss: 0.107609  [ 6464/60000]\n",
      "loss: 0.079799  [12864/60000]\n",
      "loss: 0.068641  [19264/60000]\n",
      "loss: 0.012543  [25664/60000]\n",
      "loss: 0.073814  [32064/60000]\n",
      "loss: 0.056249  [38464/60000]\n",
      "loss: 0.110039  [44864/60000]\n",
      "loss: 0.119603  [51264/60000]\n",
      "loss: 0.131005  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.064941 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.021271  [   64/60000]\n",
      "loss: 0.106552  [ 6464/60000]\n",
      "loss: 0.079521  [12864/60000]\n",
      "loss: 0.069130  [19264/60000]\n",
      "loss: 0.012306  [25664/60000]\n",
      "loss: 0.072787  [32064/60000]\n",
      "loss: 0.055778  [38464/60000]\n",
      "loss: 0.109986  [44864/60000]\n",
      "loss: 0.118301  [51264/60000]\n",
      "loss: 0.130112  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.064546 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.020957  [   64/60000]\n",
      "loss: 0.105970  [ 6464/60000]\n",
      "loss: 0.079222  [12864/60000]\n",
      "loss: 0.068501  [19264/60000]\n",
      "loss: 0.012145  [25664/60000]\n",
      "loss: 0.071742  [32064/60000]\n",
      "loss: 0.055470  [38464/60000]\n",
      "loss: 0.109382  [44864/60000]\n",
      "loss: 0.117939  [51264/60000]\n",
      "loss: 0.129285  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.064220 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.020741  [   64/60000]\n",
      "loss: 0.105365  [ 6464/60000]\n",
      "loss: 0.079246  [12864/60000]\n",
      "loss: 0.067891  [19264/60000]\n",
      "loss: 0.011831  [25664/60000]\n",
      "loss: 0.070723  [32064/60000]\n",
      "loss: 0.055176  [38464/60000]\n",
      "loss: 0.109035  [44864/60000]\n",
      "loss: 0.117731  [51264/60000]\n",
      "loss: 0.128527  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.063861 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.020514  [   64/60000]\n",
      "loss: 0.104382  [ 6464/60000]\n",
      "loss: 0.078874  [12864/60000]\n",
      "loss: 0.067051  [19264/60000]\n",
      "loss: 0.011696  [25664/60000]\n",
      "loss: 0.070251  [32064/60000]\n",
      "loss: 0.054898  [38464/60000]\n",
      "loss: 0.108173  [44864/60000]\n",
      "loss: 0.117838  [51264/60000]\n",
      "loss: 0.127489  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.063713 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.020280  [   64/60000]\n",
      "loss: 0.103529  [ 6464/60000]\n",
      "loss: 0.078618  [12864/60000]\n",
      "loss: 0.066836  [19264/60000]\n",
      "loss: 0.011504  [25664/60000]\n",
      "loss: 0.069514  [32064/60000]\n",
      "loss: 0.054578  [38464/60000]\n",
      "loss: 0.108120  [44864/60000]\n",
      "loss: 0.116746  [51264/60000]\n",
      "loss: 0.126794  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.063327 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.020031  [   64/60000]\n",
      "loss: 0.102752  [ 6464/60000]\n",
      "loss: 0.078086  [12864/60000]\n",
      "loss: 0.065893  [19264/60000]\n",
      "loss: 0.011351  [25664/60000]\n",
      "loss: 0.068726  [32064/60000]\n",
      "loss: 0.054383  [38464/60000]\n",
      "loss: 0.107366  [44864/60000]\n",
      "loss: 0.116386  [51264/60000]\n",
      "loss: 0.125955  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.063025 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.019706  [   64/60000]\n",
      "loss: 0.102461  [ 6464/60000]\n",
      "loss: 0.077733  [12864/60000]\n",
      "loss: 0.065217  [19264/60000]\n",
      "loss: 0.011225  [25664/60000]\n",
      "loss: 0.067665  [32064/60000]\n",
      "loss: 0.054134  [38464/60000]\n",
      "loss: 0.107257  [44864/60000]\n",
      "loss: 0.115803  [51264/60000]\n",
      "loss: 0.125678  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.062684 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.019501  [   64/60000]\n",
      "loss: 0.101943  [ 6464/60000]\n",
      "loss: 0.077381  [12864/60000]\n",
      "loss: 0.065174  [19264/60000]\n",
      "loss: 0.011004  [25664/60000]\n",
      "loss: 0.067243  [32064/60000]\n",
      "loss: 0.053671  [38464/60000]\n",
      "loss: 0.106570  [44864/60000]\n",
      "loss: 0.115609  [51264/60000]\n",
      "loss: 0.124956  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.062320 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.019431  [   64/60000]\n",
      "loss: 0.101414  [ 6464/60000]\n",
      "loss: 0.076692  [12864/60000]\n",
      "loss: 0.064330  [19264/60000]\n",
      "loss: 0.010858  [25664/60000]\n",
      "loss: 0.066261  [32064/60000]\n",
      "loss: 0.053425  [38464/60000]\n",
      "loss: 0.106263  [44864/60000]\n",
      "loss: 0.114986  [51264/60000]\n",
      "loss: 0.124232  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.061939 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.019077  [   64/60000]\n",
      "loss: 0.100883  [ 6464/60000]\n",
      "loss: 0.076319  [12864/60000]\n",
      "loss: 0.064007  [19264/60000]\n",
      "loss: 0.010712  [25664/60000]\n",
      "loss: 0.066201  [32064/60000]\n",
      "loss: 0.053156  [38464/60000]\n",
      "loss: 0.106304  [44864/60000]\n",
      "loss: 0.114029  [51264/60000]\n",
      "loss: 0.123633  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.061595 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.018894  [   64/60000]\n",
      "loss: 0.100765  [ 6464/60000]\n",
      "loss: 0.075663  [12864/60000]\n",
      "loss: 0.063339  [19264/60000]\n",
      "loss: 0.010589  [25664/60000]\n",
      "loss: 0.065599  [32064/60000]\n",
      "loss: 0.052777  [38464/60000]\n",
      "loss: 0.105570  [44864/60000]\n",
      "loss: 0.113597  [51264/60000]\n",
      "loss: 0.123269  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.061370 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.018635  [   64/60000]\n",
      "loss: 0.100323  [ 6464/60000]\n",
      "loss: 0.075164  [12864/60000]\n",
      "loss: 0.062504  [19264/60000]\n",
      "loss: 0.010413  [25664/60000]\n",
      "loss: 0.065243  [32064/60000]\n",
      "loss: 0.052631  [38464/60000]\n",
      "loss: 0.105002  [44864/60000]\n",
      "loss: 0.112882  [51264/60000]\n",
      "loss: 0.122665  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.061027 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.018484  [   64/60000]\n",
      "loss: 0.099772  [ 6464/60000]\n",
      "loss: 0.074608  [12864/60000]\n",
      "loss: 0.062658  [19264/60000]\n",
      "loss: 0.010228  [25664/60000]\n",
      "loss: 0.064424  [32064/60000]\n",
      "loss: 0.052131  [38464/60000]\n",
      "loss: 0.104494  [44864/60000]\n",
      "loss: 0.112362  [51264/60000]\n",
      "loss: 0.121816  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.060704 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.018220  [   64/60000]\n",
      "loss: 0.099884  [ 6464/60000]\n",
      "loss: 0.073938  [12864/60000]\n",
      "loss: 0.061908  [19264/60000]\n",
      "loss: 0.010192  [25664/60000]\n",
      "loss: 0.064047  [32064/60000]\n",
      "loss: 0.051862  [38464/60000]\n",
      "loss: 0.103726  [44864/60000]\n",
      "loss: 0.112116  [51264/60000]\n",
      "loss: 0.121262  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.060382 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.017949  [   64/60000]\n",
      "loss: 0.099627  [ 6464/60000]\n",
      "loss: 0.073317  [12864/60000]\n",
      "loss: 0.060802  [19264/60000]\n",
      "loss: 0.010080  [25664/60000]\n",
      "loss: 0.063508  [32064/60000]\n",
      "loss: 0.051451  [38464/60000]\n",
      "loss: 0.103855  [44864/60000]\n",
      "loss: 0.112140  [51264/60000]\n",
      "loss: 0.119941  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.060095 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.017879  [   64/60000]\n",
      "loss: 0.099337  [ 6464/60000]\n",
      "loss: 0.072739  [12864/60000]\n",
      "loss: 0.060936  [19264/60000]\n",
      "loss: 0.009959  [25664/60000]\n",
      "loss: 0.063024  [32064/60000]\n",
      "loss: 0.051306  [38464/60000]\n",
      "loss: 0.103394  [44864/60000]\n",
      "loss: 0.111817  [51264/60000]\n",
      "loss: 0.119608  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.059799 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.017662  [   64/60000]\n",
      "loss: 0.098751  [ 6464/60000]\n",
      "loss: 0.072142  [12864/60000]\n",
      "loss: 0.060761  [19264/60000]\n",
      "loss: 0.009808  [25664/60000]\n",
      "loss: 0.063063  [32064/60000]\n",
      "loss: 0.050919  [38464/60000]\n",
      "loss: 0.102666  [44864/60000]\n",
      "loss: 0.110985  [51264/60000]\n",
      "loss: 0.118998  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.059541 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.017531  [   64/60000]\n",
      "loss: 0.098469  [ 6464/60000]\n",
      "loss: 0.071362  [12864/60000]\n",
      "loss: 0.059856  [19264/60000]\n",
      "loss: 0.009616  [25664/60000]\n",
      "loss: 0.062548  [32064/60000]\n",
      "loss: 0.050682  [38464/60000]\n",
      "loss: 0.102520  [44864/60000]\n",
      "loss: 0.110702  [51264/60000]\n",
      "loss: 0.118794  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.059273 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.017497  [   64/60000]\n",
      "loss: 0.098071  [ 6464/60000]\n",
      "loss: 0.070634  [12864/60000]\n",
      "loss: 0.059286  [19264/60000]\n",
      "loss: 0.009560  [25664/60000]\n",
      "loss: 0.062488  [32064/60000]\n",
      "loss: 0.050562  [38464/60000]\n",
      "loss: 0.101854  [44864/60000]\n",
      "loss: 0.110068  [51264/60000]\n",
      "loss: 0.118175  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.059039 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.017325  [   64/60000]\n",
      "loss: 0.097552  [ 6464/60000]\n",
      "loss: 0.070107  [12864/60000]\n",
      "loss: 0.058515  [19264/60000]\n",
      "loss: 0.009449  [25664/60000]\n",
      "loss: 0.061914  [32064/60000]\n",
      "loss: 0.049977  [38464/60000]\n",
      "loss: 0.101637  [44864/60000]\n",
      "loss: 0.109653  [51264/60000]\n",
      "loss: 0.117711  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.058741 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.017272  [   64/60000]\n",
      "loss: 0.097368  [ 6464/60000]\n",
      "loss: 0.069286  [12864/60000]\n",
      "loss: 0.058090  [19264/60000]\n",
      "loss: 0.009422  [25664/60000]\n",
      "loss: 0.061564  [32064/60000]\n",
      "loss: 0.049638  [38464/60000]\n",
      "loss: 0.101462  [44864/60000]\n",
      "loss: 0.109564  [51264/60000]\n",
      "loss: 0.117399  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.058480 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.017148  [   64/60000]\n",
      "loss: 0.096988  [ 6464/60000]\n",
      "loss: 0.068795  [12864/60000]\n",
      "loss: 0.057489  [19264/60000]\n",
      "loss: 0.009297  [25664/60000]\n",
      "loss: 0.061208  [32064/60000]\n",
      "loss: 0.048976  [38464/60000]\n",
      "loss: 0.100994  [44864/60000]\n",
      "loss: 0.108780  [51264/60000]\n",
      "loss: 0.117114  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.058167 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.017040  [   64/60000]\n",
      "loss: 0.096501  [ 6464/60000]\n",
      "loss: 0.068073  [12864/60000]\n",
      "loss: 0.058003  [19264/60000]\n",
      "loss: 0.009168  [25664/60000]\n",
      "loss: 0.060316  [32064/60000]\n",
      "loss: 0.048282  [38464/60000]\n",
      "loss: 0.100801  [44864/60000]\n",
      "loss: 0.108320  [51264/60000]\n",
      "loss: 0.116575  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.057894 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.016812  [   64/60000]\n",
      "loss: 0.096119  [ 6464/60000]\n",
      "loss: 0.067369  [12864/60000]\n",
      "loss: 0.057743  [19264/60000]\n",
      "loss: 0.009147  [25664/60000]\n",
      "loss: 0.060035  [32064/60000]\n",
      "loss: 0.047700  [38464/60000]\n",
      "loss: 0.100336  [44864/60000]\n",
      "loss: 0.108455  [51264/60000]\n",
      "loss: 0.116011  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.057657 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.016687  [   64/60000]\n",
      "loss: 0.095885  [ 6464/60000]\n",
      "loss: 0.066622  [12864/60000]\n",
      "loss: 0.057598  [19264/60000]\n",
      "loss: 0.009082  [25664/60000]\n",
      "loss: 0.059323  [32064/60000]\n",
      "loss: 0.047158  [38464/60000]\n",
      "loss: 0.100153  [44864/60000]\n",
      "loss: 0.108247  [51264/60000]\n",
      "loss: 0.115537  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.057376 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.016571  [   64/60000]\n",
      "loss: 0.095031  [ 6464/60000]\n",
      "loss: 0.066208  [12864/60000]\n",
      "loss: 0.056866  [19264/60000]\n",
      "loss: 0.008959  [25664/60000]\n",
      "loss: 0.058916  [32064/60000]\n",
      "loss: 0.046835  [38464/60000]\n",
      "loss: 0.099975  [44864/60000]\n",
      "loss: 0.108223  [51264/60000]\n",
      "loss: 0.115117  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.057194 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.016469  [   64/60000]\n",
      "loss: 0.094882  [ 6464/60000]\n",
      "loss: 0.065479  [12864/60000]\n",
      "loss: 0.056710  [19264/60000]\n",
      "loss: 0.008879  [25664/60000]\n",
      "loss: 0.058889  [32064/60000]\n",
      "loss: 0.046473  [38464/60000]\n",
      "loss: 0.099403  [44864/60000]\n",
      "loss: 0.107743  [51264/60000]\n",
      "loss: 0.114645  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.056940 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.016396  [   64/60000]\n",
      "loss: 0.093929  [ 6464/60000]\n",
      "loss: 0.064850  [12864/60000]\n",
      "loss: 0.056168  [19264/60000]\n",
      "loss: 0.008856  [25664/60000]\n",
      "loss: 0.058417  [32064/60000]\n",
      "loss: 0.046024  [38464/60000]\n",
      "loss: 0.099254  [44864/60000]\n",
      "loss: 0.107484  [51264/60000]\n",
      "loss: 0.113994  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.056755 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.016276  [   64/60000]\n",
      "loss: 0.093453  [ 6464/60000]\n",
      "loss: 0.064338  [12864/60000]\n",
      "loss: 0.055854  [19264/60000]\n",
      "loss: 0.008795  [25664/60000]\n",
      "loss: 0.058468  [32064/60000]\n",
      "loss: 0.045589  [38464/60000]\n",
      "loss: 0.099122  [44864/60000]\n",
      "loss: 0.107020  [51264/60000]\n",
      "loss: 0.113514  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.056529 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.016254  [   64/60000]\n",
      "loss: 0.092902  [ 6464/60000]\n",
      "loss: 0.063715  [12864/60000]\n",
      "loss: 0.055458  [19264/60000]\n",
      "loss: 0.008662  [25664/60000]\n",
      "loss: 0.058146  [32064/60000]\n",
      "loss: 0.045080  [38464/60000]\n",
      "loss: 0.098914  [44864/60000]\n",
      "loss: 0.106668  [51264/60000]\n",
      "loss: 0.113065  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.056295 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.016146  [   64/60000]\n",
      "loss: 0.092507  [ 6464/60000]\n",
      "loss: 0.063197  [12864/60000]\n",
      "loss: 0.054999  [19264/60000]\n",
      "loss: 0.008626  [25664/60000]\n",
      "loss: 0.057314  [32064/60000]\n",
      "loss: 0.044573  [38464/60000]\n",
      "loss: 0.098471  [44864/60000]\n",
      "loss: 0.106278  [51264/60000]\n",
      "loss: 0.112674  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.056085 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.016242  [   64/60000]\n",
      "loss: 0.092078  [ 6464/60000]\n",
      "loss: 0.062400  [12864/60000]\n",
      "loss: 0.054511  [19264/60000]\n",
      "loss: 0.008620  [25664/60000]\n",
      "loss: 0.057587  [32064/60000]\n",
      "loss: 0.044311  [38464/60000]\n",
      "loss: 0.098519  [44864/60000]\n",
      "loss: 0.105940  [51264/60000]\n",
      "loss: 0.111847  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.055855 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.016052  [   64/60000]\n",
      "loss: 0.091735  [ 6464/60000]\n",
      "loss: 0.061818  [12864/60000]\n",
      "loss: 0.054244  [19264/60000]\n",
      "loss: 0.008573  [25664/60000]\n",
      "loss: 0.056823  [32064/60000]\n",
      "loss: 0.044141  [38464/60000]\n",
      "loss: 0.098404  [44864/60000]\n",
      "loss: 0.105634  [51264/60000]\n",
      "loss: 0.111485  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.055577 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.015969  [   64/60000]\n",
      "loss: 0.091589  [ 6464/60000]\n",
      "loss: 0.061130  [12864/60000]\n",
      "loss: 0.053863  [19264/60000]\n",
      "loss: 0.008492  [25664/60000]\n",
      "loss: 0.056575  [32064/60000]\n",
      "loss: 0.043888  [38464/60000]\n",
      "loss: 0.098091  [44864/60000]\n",
      "loss: 0.104971  [51264/60000]\n",
      "loss: 0.110772  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.055398 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.015745  [   64/60000]\n",
      "loss: 0.091004  [ 6464/60000]\n",
      "loss: 0.060656  [12864/60000]\n",
      "loss: 0.053736  [19264/60000]\n",
      "loss: 0.008443  [25664/60000]\n",
      "loss: 0.056107  [32064/60000]\n",
      "loss: 0.043540  [38464/60000]\n",
      "loss: 0.097897  [44864/60000]\n",
      "loss: 0.104846  [51264/60000]\n",
      "loss: 0.110444  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.055194 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.015615  [   64/60000]\n",
      "loss: 0.090771  [ 6464/60000]\n",
      "loss: 0.060192  [12864/60000]\n",
      "loss: 0.053271  [19264/60000]\n",
      "loss: 0.008381  [25664/60000]\n",
      "loss: 0.055914  [32064/60000]\n",
      "loss: 0.043218  [38464/60000]\n",
      "loss: 0.097239  [44864/60000]\n",
      "loss: 0.104592  [51264/60000]\n",
      "loss: 0.109816  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.054988 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.015511  [   64/60000]\n",
      "loss: 0.090672  [ 6464/60000]\n",
      "loss: 0.059826  [12864/60000]\n",
      "loss: 0.053364  [19264/60000]\n",
      "loss: 0.008354  [25664/60000]\n",
      "loss: 0.055576  [32064/60000]\n",
      "loss: 0.043280  [38464/60000]\n",
      "loss: 0.096932  [44864/60000]\n",
      "loss: 0.104332  [51264/60000]\n",
      "loss: 0.109308  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.054787 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.015571  [   64/60000]\n",
      "loss: 0.090112  [ 6464/60000]\n",
      "loss: 0.059154  [12864/60000]\n",
      "loss: 0.053266  [19264/60000]\n",
      "loss: 0.008320  [25664/60000]\n",
      "loss: 0.054923  [32064/60000]\n",
      "loss: 0.043037  [38464/60000]\n",
      "loss: 0.096497  [44864/60000]\n",
      "loss: 0.104331  [51264/60000]\n",
      "loss: 0.108911  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.054651 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.015398  [   64/60000]\n",
      "loss: 0.090063  [ 6464/60000]\n",
      "loss: 0.058838  [12864/60000]\n",
      "loss: 0.053222  [19264/60000]\n",
      "loss: 0.008273  [25664/60000]\n",
      "loss: 0.055104  [32064/60000]\n",
      "loss: 0.042920  [38464/60000]\n",
      "loss: 0.096501  [44864/60000]\n",
      "loss: 0.104244  [51264/60000]\n",
      "loss: 0.108420  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.054485 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.015269  [   64/60000]\n",
      "loss: 0.089959  [ 6464/60000]\n",
      "loss: 0.058157  [12864/60000]\n",
      "loss: 0.053019  [19264/60000]\n",
      "loss: 0.008253  [25664/60000]\n",
      "loss: 0.054310  [32064/60000]\n",
      "loss: 0.042632  [38464/60000]\n",
      "loss: 0.095818  [44864/60000]\n",
      "loss: 0.103898  [51264/60000]\n",
      "loss: 0.107833  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.054289 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.015227  [   64/60000]\n",
      "loss: 0.089236  [ 6464/60000]\n",
      "loss: 0.057479  [12864/60000]\n",
      "loss: 0.052200  [19264/60000]\n",
      "loss: 0.008182  [25664/60000]\n",
      "loss: 0.054226  [32064/60000]\n",
      "loss: 0.042837  [38464/60000]\n",
      "loss: 0.095450  [44864/60000]\n",
      "loss: 0.103697  [51264/60000]\n",
      "loss: 0.107375  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.054129 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.014946  [   64/60000]\n",
      "loss: 0.089613  [ 6464/60000]\n",
      "loss: 0.057213  [12864/60000]\n",
      "loss: 0.052361  [19264/60000]\n",
      "loss: 0.008194  [25664/60000]\n",
      "loss: 0.054124  [32064/60000]\n",
      "loss: 0.042524  [38464/60000]\n",
      "loss: 0.094912  [44864/60000]\n",
      "loss: 0.103585  [51264/60000]\n",
      "loss: 0.106491  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.053928 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.014879  [   64/60000]\n",
      "loss: 0.088929  [ 6464/60000]\n",
      "loss: 0.056819  [12864/60000]\n",
      "loss: 0.051966  [19264/60000]\n",
      "loss: 0.008143  [25664/60000]\n",
      "loss: 0.053254  [32064/60000]\n",
      "loss: 0.042491  [38464/60000]\n",
      "loss: 0.094390  [44864/60000]\n",
      "loss: 0.103827  [51264/60000]\n",
      "loss: 0.106029  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.053693 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.014736  [   64/60000]\n",
      "loss: 0.088794  [ 6464/60000]\n",
      "loss: 0.056298  [12864/60000]\n",
      "loss: 0.051803  [19264/60000]\n",
      "loss: 0.008099  [25664/60000]\n",
      "loss: 0.052865  [32064/60000]\n",
      "loss: 0.042180  [38464/60000]\n",
      "loss: 0.093961  [44864/60000]\n",
      "loss: 0.103591  [51264/60000]\n",
      "loss: 0.105452  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.053562 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.014771  [   64/60000]\n",
      "loss: 0.088398  [ 6464/60000]\n",
      "loss: 0.055909  [12864/60000]\n",
      "loss: 0.051624  [19264/60000]\n",
      "loss: 0.008016  [25664/60000]\n",
      "loss: 0.052563  [32064/60000]\n",
      "loss: 0.041803  [38464/60000]\n",
      "loss: 0.093914  [44864/60000]\n",
      "loss: 0.103577  [51264/60000]\n",
      "loss: 0.105110  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.053355 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.014674  [   64/60000]\n",
      "loss: 0.088302  [ 6464/60000]\n",
      "loss: 0.055277  [12864/60000]\n",
      "loss: 0.051381  [19264/60000]\n",
      "loss: 0.008037  [25664/60000]\n",
      "loss: 0.052362  [32064/60000]\n",
      "loss: 0.042000  [38464/60000]\n",
      "loss: 0.093294  [44864/60000]\n",
      "loss: 0.103292  [51264/60000]\n",
      "loss: 0.104343  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.053203 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.014535  [   64/60000]\n",
      "loss: 0.088005  [ 6464/60000]\n",
      "loss: 0.054852  [12864/60000]\n",
      "loss: 0.051404  [19264/60000]\n",
      "loss: 0.007926  [25664/60000]\n",
      "loss: 0.052131  [32064/60000]\n",
      "loss: 0.041588  [38464/60000]\n",
      "loss: 0.093298  [44864/60000]\n",
      "loss: 0.102922  [51264/60000]\n",
      "loss: 0.103708  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.053049 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.014534  [   64/60000]\n",
      "loss: 0.087618  [ 6464/60000]\n",
      "loss: 0.054445  [12864/60000]\n",
      "loss: 0.050984  [19264/60000]\n",
      "loss: 0.007964  [25664/60000]\n",
      "loss: 0.051734  [32064/60000]\n",
      "loss: 0.041906  [38464/60000]\n",
      "loss: 0.093202  [44864/60000]\n",
      "loss: 0.102775  [51264/60000]\n",
      "loss: 0.103182  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.052879 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.014544  [   64/60000]\n",
      "loss: 0.087488  [ 6464/60000]\n",
      "loss: 0.054028  [12864/60000]\n",
      "loss: 0.049970  [19264/60000]\n",
      "loss: 0.007844  [25664/60000]\n",
      "loss: 0.051328  [32064/60000]\n",
      "loss: 0.041542  [38464/60000]\n",
      "loss: 0.092971  [44864/60000]\n",
      "loss: 0.102463  [51264/60000]\n",
      "loss: 0.102566  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.052701 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.014425  [   64/60000]\n",
      "loss: 0.087638  [ 6464/60000]\n",
      "loss: 0.053538  [12864/60000]\n",
      "loss: 0.049497  [19264/60000]\n",
      "loss: 0.007846  [25664/60000]\n",
      "loss: 0.051347  [32064/60000]\n",
      "loss: 0.041470  [38464/60000]\n",
      "loss: 0.092922  [44864/60000]\n",
      "loss: 0.102198  [51264/60000]\n",
      "loss: 0.101813  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.052572 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.014441  [   64/60000]\n",
      "loss: 0.087508  [ 6464/60000]\n",
      "loss: 0.053090  [12864/60000]\n",
      "loss: 0.049389  [19264/60000]\n",
      "loss: 0.007806  [25664/60000]\n",
      "loss: 0.050997  [32064/60000]\n",
      "loss: 0.041507  [38464/60000]\n",
      "loss: 0.092436  [44864/60000]\n",
      "loss: 0.101611  [51264/60000]\n",
      "loss: 0.101387  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.052454 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.014383  [   64/60000]\n",
      "loss: 0.086985  [ 6464/60000]\n",
      "loss: 0.052742  [12864/60000]\n",
      "loss: 0.048862  [19264/60000]\n",
      "loss: 0.007794  [25664/60000]\n",
      "loss: 0.050805  [32064/60000]\n",
      "loss: 0.041324  [38464/60000]\n",
      "loss: 0.092313  [44864/60000]\n",
      "loss: 0.101606  [51264/60000]\n",
      "loss: 0.100763  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.052172 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.014319  [   64/60000]\n",
      "loss: 0.086102  [ 6464/60000]\n",
      "loss: 0.052288  [12864/60000]\n",
      "loss: 0.048323  [19264/60000]\n",
      "loss: 0.007749  [25664/60000]\n",
      "loss: 0.049857  [32064/60000]\n",
      "loss: 0.040936  [38464/60000]\n",
      "loss: 0.091586  [44864/60000]\n",
      "loss: 0.102171  [51264/60000]\n",
      "loss: 0.100217  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.052075 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.014371  [   64/60000]\n",
      "loss: 0.087367  [ 6464/60000]\n",
      "loss: 0.051681  [12864/60000]\n",
      "loss: 0.048385  [19264/60000]\n",
      "loss: 0.007778  [25664/60000]\n",
      "loss: 0.049442  [32064/60000]\n",
      "loss: 0.041276  [38464/60000]\n",
      "loss: 0.091521  [44864/60000]\n",
      "loss: 0.101355  [51264/60000]\n",
      "loss: 0.099772  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.051919 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.014266  [   64/60000]\n",
      "loss: 0.087248  [ 6464/60000]\n",
      "loss: 0.051355  [12864/60000]\n",
      "loss: 0.048310  [19264/60000]\n",
      "loss: 0.007749  [25664/60000]\n",
      "loss: 0.049658  [32064/60000]\n",
      "loss: 0.041087  [38464/60000]\n",
      "loss: 0.091011  [44864/60000]\n",
      "loss: 0.101615  [51264/60000]\n",
      "loss: 0.099419  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.051751 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.014315  [   64/60000]\n",
      "loss: 0.086859  [ 6464/60000]\n",
      "loss: 0.050926  [12864/60000]\n",
      "loss: 0.048126  [19264/60000]\n",
      "loss: 0.007664  [25664/60000]\n",
      "loss: 0.049585  [32064/60000]\n",
      "loss: 0.040802  [38464/60000]\n",
      "loss: 0.090912  [44864/60000]\n",
      "loss: 0.101416  [51264/60000]\n",
      "loss: 0.098603  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.051634 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.014164  [   64/60000]\n",
      "loss: 0.086408  [ 6464/60000]\n",
      "loss: 0.050418  [12864/60000]\n",
      "loss: 0.047575  [19264/60000]\n",
      "loss: 0.007678  [25664/60000]\n",
      "loss: 0.049028  [32064/60000]\n",
      "loss: 0.040899  [38464/60000]\n",
      "loss: 0.090597  [44864/60000]\n",
      "loss: 0.101088  [51264/60000]\n",
      "loss: 0.098362  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.051491 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.014267  [   64/60000]\n",
      "loss: 0.086285  [ 6464/60000]\n",
      "loss: 0.049881  [12864/60000]\n",
      "loss: 0.047724  [19264/60000]\n",
      "loss: 0.007651  [25664/60000]\n",
      "loss: 0.048739  [32064/60000]\n",
      "loss: 0.040670  [38464/60000]\n",
      "loss: 0.090145  [44864/60000]\n",
      "loss: 0.101352  [51264/60000]\n",
      "loss: 0.097795  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.051295 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.014205  [   64/60000]\n",
      "loss: 0.085573  [ 6464/60000]\n",
      "loss: 0.049484  [12864/60000]\n",
      "loss: 0.047364  [19264/60000]\n",
      "loss: 0.007678  [25664/60000]\n",
      "loss: 0.049064  [32064/60000]\n",
      "loss: 0.040626  [38464/60000]\n",
      "loss: 0.089971  [44864/60000]\n",
      "loss: 0.100883  [51264/60000]\n",
      "loss: 0.097435  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.051221 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.014164  [   64/60000]\n",
      "loss: 0.085906  [ 6464/60000]\n",
      "loss: 0.048953  [12864/60000]\n",
      "loss: 0.047607  [19264/60000]\n",
      "loss: 0.007640  [25664/60000]\n",
      "loss: 0.048414  [32064/60000]\n",
      "loss: 0.040560  [38464/60000]\n",
      "loss: 0.089761  [44864/60000]\n",
      "loss: 0.101192  [51264/60000]\n",
      "loss: 0.096773  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.051036 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.014117  [   64/60000]\n",
      "loss: 0.085502  [ 6464/60000]\n",
      "loss: 0.048388  [12864/60000]\n",
      "loss: 0.046752  [19264/60000]\n",
      "loss: 0.007575  [25664/60000]\n",
      "loss: 0.047898  [32064/60000]\n",
      "loss: 0.040299  [38464/60000]\n",
      "loss: 0.088957  [44864/60000]\n",
      "loss: 0.100866  [51264/60000]\n",
      "loss: 0.096185  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.050923 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.014177  [   64/60000]\n",
      "loss: 0.085581  [ 6464/60000]\n",
      "loss: 0.047791  [12864/60000]\n",
      "loss: 0.047050  [19264/60000]\n",
      "loss: 0.007560  [25664/60000]\n",
      "loss: 0.047361  [32064/60000]\n",
      "loss: 0.040228  [38464/60000]\n",
      "loss: 0.088693  [44864/60000]\n",
      "loss: 0.100688  [51264/60000]\n",
      "loss: 0.095504  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.050814 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.014103  [   64/60000]\n",
      "loss: 0.085569  [ 6464/60000]\n",
      "loss: 0.047405  [12864/60000]\n",
      "loss: 0.046145  [19264/60000]\n",
      "loss: 0.007572  [25664/60000]\n",
      "loss: 0.047318  [32064/60000]\n",
      "loss: 0.039969  [38464/60000]\n",
      "loss: 0.088151  [44864/60000]\n",
      "loss: 0.101005  [51264/60000]\n",
      "loss: 0.095343  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.050672 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.014064  [   64/60000]\n",
      "loss: 0.085520  [ 6464/60000]\n",
      "loss: 0.046823  [12864/60000]\n",
      "loss: 0.046036  [19264/60000]\n",
      "loss: 0.007537  [25664/60000]\n",
      "loss: 0.047497  [32064/60000]\n",
      "loss: 0.040132  [38464/60000]\n",
      "loss: 0.088059  [44864/60000]\n",
      "loss: 0.100545  [51264/60000]\n",
      "loss: 0.094662  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.050532 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.013973  [   64/60000]\n",
      "loss: 0.085114  [ 6464/60000]\n",
      "loss: 0.046217  [12864/60000]\n",
      "loss: 0.045970  [19264/60000]\n",
      "loss: 0.007538  [25664/60000]\n",
      "loss: 0.047136  [32064/60000]\n",
      "loss: 0.039882  [38464/60000]\n",
      "loss: 0.087635  [44864/60000]\n",
      "loss: 0.100455  [51264/60000]\n",
      "loss: 0.094117  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.050400 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.013946  [   64/60000]\n",
      "loss: 0.084928  [ 6464/60000]\n",
      "loss: 0.045741  [12864/60000]\n",
      "loss: 0.045651  [19264/60000]\n",
      "loss: 0.007464  [25664/60000]\n",
      "loss: 0.046226  [32064/60000]\n",
      "loss: 0.039580  [38464/60000]\n",
      "loss: 0.087306  [44864/60000]\n",
      "loss: 0.100516  [51264/60000]\n",
      "loss: 0.093710  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.050235 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.013979  [   64/60000]\n",
      "loss: 0.084828  [ 6464/60000]\n",
      "loss: 0.045119  [12864/60000]\n",
      "loss: 0.045758  [19264/60000]\n",
      "loss: 0.007483  [25664/60000]\n",
      "loss: 0.046179  [32064/60000]\n",
      "loss: 0.039576  [38464/60000]\n",
      "loss: 0.087342  [44864/60000]\n",
      "loss: 0.100367  [51264/60000]\n",
      "loss: 0.093104  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.050106 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.013910  [   64/60000]\n",
      "loss: 0.084593  [ 6464/60000]\n",
      "loss: 0.044805  [12864/60000]\n",
      "loss: 0.045357  [19264/60000]\n",
      "loss: 0.007463  [25664/60000]\n",
      "loss: 0.045706  [32064/60000]\n",
      "loss: 0.039220  [38464/60000]\n",
      "loss: 0.086972  [44864/60000]\n",
      "loss: 0.100118  [51264/60000]\n",
      "loss: 0.092738  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.049956 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.014014  [   64/60000]\n",
      "loss: 0.084432  [ 6464/60000]\n",
      "loss: 0.044452  [12864/60000]\n",
      "loss: 0.044825  [19264/60000]\n",
      "loss: 0.007413  [25664/60000]\n",
      "loss: 0.045218  [32064/60000]\n",
      "loss: 0.038934  [38464/60000]\n",
      "loss: 0.086576  [44864/60000]\n",
      "loss: 0.100334  [51264/60000]\n",
      "loss: 0.092153  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.049872 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.014149  [   64/60000]\n",
      "loss: 0.084312  [ 6464/60000]\n",
      "loss: 0.044033  [12864/60000]\n",
      "loss: 0.044565  [19264/60000]\n",
      "loss: 0.007343  [25664/60000]\n",
      "loss: 0.044453  [32064/60000]\n",
      "loss: 0.038942  [38464/60000]\n",
      "loss: 0.086384  [44864/60000]\n",
      "loss: 0.100063  [51264/60000]\n",
      "loss: 0.091503  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049712 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.014114  [   64/60000]\n",
      "loss: 0.083650  [ 6464/60000]\n",
      "loss: 0.043703  [12864/60000]\n",
      "loss: 0.044259  [19264/60000]\n",
      "loss: 0.007346  [25664/60000]\n",
      "loss: 0.044440  [32064/60000]\n",
      "loss: 0.038761  [38464/60000]\n",
      "loss: 0.085751  [44864/60000]\n",
      "loss: 0.100118  [51264/60000]\n",
      "loss: 0.091198  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049593 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.014056  [   64/60000]\n",
      "loss: 0.083275  [ 6464/60000]\n",
      "loss: 0.043247  [12864/60000]\n",
      "loss: 0.043597  [19264/60000]\n",
      "loss: 0.007317  [25664/60000]\n",
      "loss: 0.043724  [32064/60000]\n",
      "loss: 0.038653  [38464/60000]\n",
      "loss: 0.085384  [44864/60000]\n",
      "loss: 0.100114  [51264/60000]\n",
      "loss: 0.090937  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049482 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.014180  [   64/60000]\n",
      "loss: 0.083134  [ 6464/60000]\n",
      "loss: 0.042928  [12864/60000]\n",
      "loss: 0.043828  [19264/60000]\n",
      "loss: 0.007297  [25664/60000]\n",
      "loss: 0.043731  [32064/60000]\n",
      "loss: 0.038605  [38464/60000]\n",
      "loss: 0.084962  [44864/60000]\n",
      "loss: 0.099913  [51264/60000]\n",
      "loss: 0.090309  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049399 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.014220  [   64/60000]\n",
      "loss: 0.082910  [ 6464/60000]\n",
      "loss: 0.042766  [12864/60000]\n",
      "loss: 0.043384  [19264/60000]\n",
      "loss: 0.007292  [25664/60000]\n",
      "loss: 0.043053  [32064/60000]\n",
      "loss: 0.038402  [38464/60000]\n",
      "loss: 0.084636  [44864/60000]\n",
      "loss: 0.099667  [51264/60000]\n",
      "loss: 0.089855  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049223 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.014321  [   64/60000]\n",
      "loss: 0.082570  [ 6464/60000]\n",
      "loss: 0.042344  [12864/60000]\n",
      "loss: 0.042996  [19264/60000]\n",
      "loss: 0.007245  [25664/60000]\n",
      "loss: 0.042910  [32064/60000]\n",
      "loss: 0.038272  [38464/60000]\n",
      "loss: 0.084502  [44864/60000]\n",
      "loss: 0.099167  [51264/60000]\n",
      "loss: 0.089399  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049197 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.014255  [   64/60000]\n",
      "loss: 0.082640  [ 6464/60000]\n",
      "loss: 0.042053  [12864/60000]\n",
      "loss: 0.042178  [19264/60000]\n",
      "loss: 0.007194  [25664/60000]\n",
      "loss: 0.042586  [32064/60000]\n",
      "loss: 0.037926  [38464/60000]\n",
      "loss: 0.084365  [44864/60000]\n",
      "loss: 0.099235  [51264/60000]\n",
      "loss: 0.089268  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049028 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.014393  [   64/60000]\n",
      "loss: 0.082327  [ 6464/60000]\n",
      "loss: 0.041379  [12864/60000]\n",
      "loss: 0.042243  [19264/60000]\n",
      "loss: 0.007186  [25664/60000]\n",
      "loss: 0.042337  [32064/60000]\n",
      "loss: 0.038071  [38464/60000]\n",
      "loss: 0.083787  [44864/60000]\n",
      "loss: 0.098707  [51264/60000]\n",
      "loss: 0.088600  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048965 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.014217  [   64/60000]\n",
      "loss: 0.082086  [ 6464/60000]\n",
      "loss: 0.041028  [12864/60000]\n",
      "loss: 0.041450  [19264/60000]\n",
      "loss: 0.007114  [25664/60000]\n",
      "loss: 0.041933  [32064/60000]\n",
      "loss: 0.037708  [38464/60000]\n",
      "loss: 0.083320  [44864/60000]\n",
      "loss: 0.098623  [51264/60000]\n",
      "loss: 0.088321  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.048781 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.014318  [   64/60000]\n",
      "loss: 0.081601  [ 6464/60000]\n",
      "loss: 0.041058  [12864/60000]\n",
      "loss: 0.041646  [19264/60000]\n",
      "loss: 0.007105  [25664/60000]\n",
      "loss: 0.041370  [32064/60000]\n",
      "loss: 0.037837  [38464/60000]\n",
      "loss: 0.083205  [44864/60000]\n",
      "loss: 0.098334  [51264/60000]\n",
      "loss: 0.087924  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048731 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.014203  [   64/60000]\n",
      "loss: 0.081863  [ 6464/60000]\n",
      "loss: 0.040503  [12864/60000]\n",
      "loss: 0.041152  [19264/60000]\n",
      "loss: 0.007081  [25664/60000]\n",
      "loss: 0.041082  [32064/60000]\n",
      "loss: 0.037647  [38464/60000]\n",
      "loss: 0.082767  [44864/60000]\n",
      "loss: 0.098286  [51264/60000]\n",
      "loss: 0.087364  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048614 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.014273  [   64/60000]\n",
      "loss: 0.081392  [ 6464/60000]\n",
      "loss: 0.040107  [12864/60000]\n",
      "loss: 0.041175  [19264/60000]\n",
      "loss: 0.007055  [25664/60000]\n",
      "loss: 0.041411  [32064/60000]\n",
      "loss: 0.037826  [38464/60000]\n",
      "loss: 0.082969  [44864/60000]\n",
      "loss: 0.098254  [51264/60000]\n",
      "loss: 0.086904  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048489 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.014198  [   64/60000]\n",
      "loss: 0.081054  [ 6464/60000]\n",
      "loss: 0.039846  [12864/60000]\n",
      "loss: 0.040893  [19264/60000]\n",
      "loss: 0.007017  [25664/60000]\n",
      "loss: 0.041247  [32064/60000]\n",
      "loss: 0.037724  [38464/60000]\n",
      "loss: 0.082657  [44864/60000]\n",
      "loss: 0.097536  [51264/60000]\n",
      "loss: 0.086250  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048435 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.014166  [   64/60000]\n",
      "loss: 0.081342  [ 6464/60000]\n",
      "loss: 0.039221  [12864/60000]\n",
      "loss: 0.040774  [19264/60000]\n",
      "loss: 0.006977  [25664/60000]\n",
      "loss: 0.040726  [32064/60000]\n",
      "loss: 0.037539  [38464/60000]\n",
      "loss: 0.082512  [44864/60000]\n",
      "loss: 0.097463  [51264/60000]\n",
      "loss: 0.085885  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048215 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.014144  [   64/60000]\n",
      "loss: 0.080568  [ 6464/60000]\n",
      "loss: 0.038945  [12864/60000]\n",
      "loss: 0.040434  [19264/60000]\n",
      "loss: 0.006998  [25664/60000]\n",
      "loss: 0.040221  [32064/60000]\n",
      "loss: 0.037367  [38464/60000]\n",
      "loss: 0.082162  [44864/60000]\n",
      "loss: 0.097199  [51264/60000]\n",
      "loss: 0.085473  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048187 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.014252  [   64/60000]\n",
      "loss: 0.080473  [ 6464/60000]\n",
      "loss: 0.038719  [12864/60000]\n",
      "loss: 0.040579  [19264/60000]\n",
      "loss: 0.006962  [25664/60000]\n",
      "loss: 0.039907  [32064/60000]\n",
      "loss: 0.037115  [38464/60000]\n",
      "loss: 0.082487  [44864/60000]\n",
      "loss: 0.097130  [51264/60000]\n",
      "loss: 0.085103  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048061 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.014252  [   64/60000]\n",
      "loss: 0.079731  [ 6464/60000]\n",
      "loss: 0.038490  [12864/60000]\n",
      "loss: 0.039691  [19264/60000]\n",
      "loss: 0.006954  [25664/60000]\n",
      "loss: 0.039498  [32064/60000]\n",
      "loss: 0.037067  [38464/60000]\n",
      "loss: 0.082178  [44864/60000]\n",
      "loss: 0.097184  [51264/60000]\n",
      "loss: 0.084524  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.047997 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.014316  [   64/60000]\n",
      "loss: 0.079972  [ 6464/60000]\n",
      "loss: 0.038230  [12864/60000]\n",
      "loss: 0.039814  [19264/60000]\n",
      "loss: 0.006930  [25664/60000]\n",
      "loss: 0.039594  [32064/60000]\n",
      "loss: 0.036917  [38464/60000]\n",
      "loss: 0.081726  [44864/60000]\n",
      "loss: 0.096762  [51264/60000]\n",
      "loss: 0.084225  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047829 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.014338  [   64/60000]\n",
      "loss: 0.079007  [ 6464/60000]\n",
      "loss: 0.037754  [12864/60000]\n",
      "loss: 0.039691  [19264/60000]\n",
      "loss: 0.006906  [25664/60000]\n",
      "loss: 0.039060  [32064/60000]\n",
      "loss: 0.036802  [38464/60000]\n",
      "loss: 0.081696  [44864/60000]\n",
      "loss: 0.096820  [51264/60000]\n",
      "loss: 0.083644  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047783 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.014282  [   64/60000]\n",
      "loss: 0.079359  [ 6464/60000]\n",
      "loss: 0.037421  [12864/60000]\n",
      "loss: 0.039448  [19264/60000]\n",
      "loss: 0.006885  [25664/60000]\n",
      "loss: 0.038303  [32064/60000]\n",
      "loss: 0.036735  [38464/60000]\n",
      "loss: 0.081543  [44864/60000]\n",
      "loss: 0.096617  [51264/60000]\n",
      "loss: 0.083089  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047722 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.014369  [   64/60000]\n",
      "loss: 0.079497  [ 6464/60000]\n",
      "loss: 0.036839  [12864/60000]\n",
      "loss: 0.038992  [19264/60000]\n",
      "loss: 0.006881  [25664/60000]\n",
      "loss: 0.038345  [32064/60000]\n",
      "loss: 0.036428  [38464/60000]\n",
      "loss: 0.081255  [44864/60000]\n",
      "loss: 0.096482  [51264/60000]\n",
      "loss: 0.083056  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047564 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.014447  [   64/60000]\n",
      "loss: 0.078766  [ 6464/60000]\n",
      "loss: 0.036801  [12864/60000]\n",
      "loss: 0.038669  [19264/60000]\n",
      "loss: 0.006835  [25664/60000]\n",
      "loss: 0.038575  [32064/60000]\n",
      "loss: 0.036503  [38464/60000]\n",
      "loss: 0.081376  [44864/60000]\n",
      "loss: 0.096605  [51264/60000]\n",
      "loss: 0.082188  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047505 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.014459  [   64/60000]\n",
      "loss: 0.077916  [ 6464/60000]\n",
      "loss: 0.036252  [12864/60000]\n",
      "loss: 0.038721  [19264/60000]\n",
      "loss: 0.006822  [25664/60000]\n",
      "loss: 0.038381  [32064/60000]\n",
      "loss: 0.036186  [38464/60000]\n",
      "loss: 0.081124  [44864/60000]\n",
      "loss: 0.096354  [51264/60000]\n",
      "loss: 0.081955  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047362 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.014484  [   64/60000]\n",
      "loss: 0.077735  [ 6464/60000]\n",
      "loss: 0.035873  [12864/60000]\n",
      "loss: 0.038543  [19264/60000]\n",
      "loss: 0.006849  [25664/60000]\n",
      "loss: 0.037733  [32064/60000]\n",
      "loss: 0.036119  [38464/60000]\n",
      "loss: 0.080628  [44864/60000]\n",
      "loss: 0.096540  [51264/60000]\n",
      "loss: 0.081707  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047310 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.014557  [   64/60000]\n",
      "loss: 0.077917  [ 6464/60000]\n",
      "loss: 0.035562  [12864/60000]\n",
      "loss: 0.038246  [19264/60000]\n",
      "loss: 0.006812  [25664/60000]\n",
      "loss: 0.037251  [32064/60000]\n",
      "loss: 0.036028  [38464/60000]\n",
      "loss: 0.080497  [44864/60000]\n",
      "loss: 0.096544  [51264/60000]\n",
      "loss: 0.081458  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047200 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.014541  [   64/60000]\n",
      "loss: 0.077652  [ 6464/60000]\n",
      "loss: 0.035232  [12864/60000]\n",
      "loss: 0.038381  [19264/60000]\n",
      "loss: 0.006793  [25664/60000]\n",
      "loss: 0.037434  [32064/60000]\n",
      "loss: 0.036036  [38464/60000]\n",
      "loss: 0.080337  [44864/60000]\n",
      "loss: 0.096406  [51264/60000]\n",
      "loss: 0.080931  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047132 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.014502  [   64/60000]\n",
      "loss: 0.076879  [ 6464/60000]\n",
      "loss: 0.034925  [12864/60000]\n",
      "loss: 0.037824  [19264/60000]\n",
      "loss: 0.006799  [25664/60000]\n",
      "loss: 0.037009  [32064/60000]\n",
      "loss: 0.035958  [38464/60000]\n",
      "loss: 0.080257  [44864/60000]\n",
      "loss: 0.096166  [51264/60000]\n",
      "loss: 0.080375  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.047041 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.014514  [   64/60000]\n",
      "loss: 0.076753  [ 6464/60000]\n",
      "loss: 0.034660  [12864/60000]\n",
      "loss: 0.037576  [19264/60000]\n",
      "loss: 0.006770  [25664/60000]\n",
      "loss: 0.036653  [32064/60000]\n",
      "loss: 0.035823  [38464/60000]\n",
      "loss: 0.080112  [44864/60000]\n",
      "loss: 0.096458  [51264/60000]\n",
      "loss: 0.079861  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.046977 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.014503  [   64/60000]\n",
      "loss: 0.076983  [ 6464/60000]\n",
      "loss: 0.034310  [12864/60000]\n",
      "loss: 0.037360  [19264/60000]\n",
      "loss: 0.006769  [25664/60000]\n",
      "loss: 0.036009  [32064/60000]\n",
      "loss: 0.035794  [38464/60000]\n",
      "loss: 0.079480  [44864/60000]\n",
      "loss: 0.096210  [51264/60000]\n",
      "loss: 0.079396  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.046906 \n",
      "\n",
      "Done in  1647.9393479824066  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "epoch = 0\n",
    "epochs = 200\n",
    "test(test_dataloader, model, loss_fn, epoch)\n",
    "for t in range(epochs):\n",
    "    epoch += 1\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, epoch, writer=writer)\n",
    "    test(test_dataloader, model, loss_fn, epoch, writer=writer)\n",
    "print(\"Done in \", time.time() - start, \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
