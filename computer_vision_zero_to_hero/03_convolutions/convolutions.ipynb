{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6212c85-79d7-485d-a729-c1f743a4a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f61d19a9-721e-44df-8c26-62036c6df4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a9c5627-defb-4261-93ab-2eea82f82a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "948af954-1a48-492e-bfbe-2a378f221715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image(image, label):\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.show()\n",
    "show_image(X[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcca71b1-6661-4907-996e-0e76c825adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b99df5eb-2171-41c5-bd6f-58f5332c52e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8863e-02, -1.0314e-01,  8.2143e-02, -1.7942e-02,  8.6686e-02,\n",
       "         -5.9447e-02, -8.2187e-03,  5.6793e-03, -4.6130e-02,  1.9320e-02],\n",
       "        [ 6.1856e-02, -8.2125e-02,  8.1672e-02, -1.2610e-02,  8.2788e-02,\n",
       "         -5.7773e-02, -8.1496e-03,  2.9086e-02, -4.4689e-02,  3.6231e-02],\n",
       "        [ 6.5731e-02, -1.0123e-01,  7.9953e-02, -2.3280e-02,  8.0531e-02,\n",
       "         -5.1576e-02, -2.0550e-02,  1.2715e-02, -5.0640e-02,  1.9248e-02],\n",
       "        [ 6.3566e-02, -1.0159e-01,  8.6879e-02, -1.7991e-02,  8.1387e-02,\n",
       "         -5.5076e-02, -5.9179e-03,  2.3423e-02, -4.6447e-02,  2.4943e-02],\n",
       "        [ 6.6535e-02, -1.0185e-01,  8.3070e-02, -1.7202e-02,  7.7287e-02,\n",
       "         -6.2211e-02, -4.5690e-03,  1.4121e-02, -4.7804e-02,  2.6989e-02],\n",
       "        [ 6.5003e-02, -1.0309e-01,  7.6233e-02, -2.5040e-02,  7.9330e-02,\n",
       "         -5.1669e-02, -2.2497e-02,  8.5425e-03, -5.3132e-02,  2.5035e-02],\n",
       "        [ 6.6501e-02, -1.0494e-01,  7.3791e-02, -2.1337e-02,  8.2009e-02,\n",
       "         -5.8077e-02, -5.6123e-03,  4.1970e-03, -4.3959e-02,  3.9395e-02],\n",
       "        [ 6.4906e-02, -1.0750e-01,  7.1826e-02, -1.3879e-02,  7.5834e-02,\n",
       "         -6.2127e-02,  3.0621e-04,  4.7935e-03, -5.8338e-02,  3.1066e-02],\n",
       "        [ 6.7204e-02, -9.3203e-02,  8.5805e-02, -2.2842e-02,  8.0998e-02,\n",
       "         -5.6611e-02, -5.0093e-03,  1.8972e-02, -4.9365e-02,  2.7797e-02],\n",
       "        [ 6.5844e-02, -1.1823e-01,  8.0039e-02, -1.4946e-02,  8.8673e-02,\n",
       "         -5.4580e-02,  9.9180e-03,  9.8942e-03, -6.3857e-02,  2.8098e-02],\n",
       "        [ 5.6148e-02, -8.3807e-02,  7.7859e-02, -3.8888e-03,  8.3209e-02,\n",
       "         -6.3542e-02,  2.1338e-05,  3.1327e-02, -4.5840e-02,  2.8088e-02],\n",
       "        [ 5.2230e-02, -1.0763e-01,  6.8777e-02, -1.1434e-02,  8.0226e-02,\n",
       "         -6.0004e-02, -7.5252e-03,  1.2708e-02, -4.6122e-02,  2.7933e-02],\n",
       "        [ 6.1124e-02, -1.1819e-01,  8.0255e-02, -2.0293e-02,  8.5278e-02,\n",
       "         -6.0307e-02, -3.6360e-03,  2.7991e-03, -5.5589e-02,  1.8019e-02],\n",
       "        [ 6.3338e-02, -9.1251e-02,  8.0513e-02, -7.5784e-03,  8.3847e-02,\n",
       "         -5.9579e-02, -8.0977e-03,  2.2985e-02, -4.6054e-02,  2.9196e-02],\n",
       "        [ 5.8974e-02, -9.9418e-02,  7.8001e-02, -2.6920e-02,  8.0013e-02,\n",
       "         -5.1574e-02, -1.4613e-02,  1.0789e-02, -5.5544e-02,  2.2481e-02],\n",
       "        [ 6.3479e-02, -9.8378e-02,  7.0375e-02, -1.6762e-02,  7.9634e-02,\n",
       "         -6.1286e-02, -1.2690e-02,  1.5150e-02, -4.9512e-02,  3.5789e-02],\n",
       "        [ 6.9621e-02, -1.0532e-01,  8.9228e-02, -1.9101e-02,  7.8080e-02,\n",
       "         -5.7768e-02, -7.3958e-03,  6.1132e-03, -4.6220e-02,  2.7423e-02],\n",
       "        [ 5.7533e-02, -1.0286e-01,  8.1774e-02, -1.9388e-02,  8.3314e-02,\n",
       "         -5.7919e-02, -1.2463e-02,  6.1420e-03, -4.1673e-02,  1.7382e-02],\n",
       "        [ 5.7368e-02, -1.0684e-01,  7.0230e-02, -1.5933e-02,  7.5909e-02,\n",
       "         -5.2889e-02, -2.6455e-03,  7.1004e-03, -5.7203e-02,  3.6775e-02],\n",
       "        [ 6.7504e-02, -1.1169e-01,  7.9231e-02, -2.3478e-02,  7.8332e-02,\n",
       "         -6.0221e-02, -7.6460e-03,  1.2983e-02, -5.3041e-02,  3.0561e-02],\n",
       "        [ 6.2734e-02, -1.1491e-01,  7.0262e-02, -2.2242e-02,  8.4119e-02,\n",
       "         -5.0654e-02, -5.3405e-03,  4.0415e-03, -5.9010e-02,  3.1844e-02],\n",
       "        [ 5.6250e-02, -1.0732e-01,  8.0106e-02, -1.9546e-02,  7.9697e-02,\n",
       "         -5.8428e-02,  1.7990e-04,  5.8818e-03, -5.4874e-02,  2.7964e-02],\n",
       "        [ 6.0238e-02, -1.0512e-01,  7.5968e-02, -1.0582e-02,  7.8312e-02,\n",
       "         -6.4189e-02, -2.7942e-03,  8.9131e-03, -5.2084e-02,  3.1085e-02],\n",
       "        [ 5.5089e-02, -9.9792e-02,  8.6929e-02, -2.7674e-02,  8.0224e-02,\n",
       "         -5.5033e-02, -6.4513e-03,  2.3507e-02, -5.5865e-02,  2.0015e-02],\n",
       "        [ 6.4222e-02, -1.0264e-01,  7.4987e-02, -2.0264e-02,  8.1597e-02,\n",
       "         -6.5339e-02, -1.1342e-02,  1.2985e-02, -5.0094e-02,  2.3133e-02],\n",
       "        [ 6.4402e-02, -1.0092e-01,  8.1146e-02, -1.2364e-02,  8.4359e-02,\n",
       "         -6.3042e-02,  7.2385e-03,  2.3659e-02, -5.3294e-02,  3.0214e-02],\n",
       "        [ 6.4664e-02, -1.0343e-01,  8.0280e-02, -2.1317e-02,  8.1276e-02,\n",
       "         -6.3753e-02, -1.7382e-02, -2.8807e-03, -4.2804e-02,  2.9996e-02],\n",
       "        [ 6.4048e-02, -1.0951e-01,  8.3387e-02, -2.0356e-02,  7.9478e-02,\n",
       "         -6.2655e-02, -8.4857e-03,  1.1353e-02, -5.2724e-02,  2.5983e-02],\n",
       "        [ 6.0633e-02, -9.3787e-02,  7.7602e-02, -1.2120e-02,  8.6439e-02,\n",
       "         -5.6788e-02, -1.2601e-02,  2.1616e-02, -4.3330e-02,  2.9483e-02],\n",
       "        [ 6.0787e-02, -9.5042e-02,  8.1897e-02, -2.8614e-02,  7.9757e-02,\n",
       "         -5.4429e-02, -1.4363e-02,  5.4466e-03, -4.9630e-02,  2.0063e-02],\n",
       "        [ 5.5932e-02, -9.9230e-02,  7.5372e-02, -2.0949e-02,  7.7632e-02,\n",
       "         -5.5832e-02, -8.2823e-03,  1.8512e-02, -5.2323e-02,  2.8423e-02],\n",
       "        [ 6.2876e-02, -9.9300e-02,  7.9637e-02, -2.7488e-02,  8.2599e-02,\n",
       "         -5.8545e-02, -1.3683e-02,  4.1592e-03, -5.0097e-02,  2.1183e-02],\n",
       "        [ 6.9116e-02, -9.7099e-02,  7.3152e-02, -1.2260e-02,  8.1675e-02,\n",
       "         -6.5066e-02,  1.0712e-03,  1.5783e-02, -5.1651e-02,  3.4666e-02],\n",
       "        [ 6.4410e-02, -1.0129e-01,  8.4746e-02, -1.5658e-02,  7.9297e-02,\n",
       "         -6.1563e-02, -4.2292e-03,  1.7335e-02, -4.7014e-02,  2.3328e-02],\n",
       "        [ 5.1571e-02, -1.0085e-01,  8.2381e-02, -9.8652e-03,  8.6323e-02,\n",
       "         -5.1678e-02, -4.7460e-03,  1.8640e-02, -5.0722e-02,  1.5618e-02],\n",
       "        [ 5.7314e-02, -8.2591e-02,  8.5184e-02, -1.0927e-02,  8.1589e-02,\n",
       "         -5.8679e-02, -1.6458e-03,  4.0316e-02, -5.0373e-02,  2.3357e-02],\n",
       "        [ 5.7402e-02, -1.0171e-01,  8.2119e-02, -1.7688e-02,  8.5910e-02,\n",
       "         -6.0852e-02, -9.5420e-03,  1.2632e-02, -3.9941e-02,  1.5520e-02],\n",
       "        [ 6.1325e-02, -9.7263e-02,  8.0095e-02, -2.8606e-02,  8.1977e-02,\n",
       "         -5.6121e-02, -1.2369e-02,  5.6360e-03, -5.0731e-02,  1.8102e-02],\n",
       "        [ 5.2088e-02, -8.8737e-02,  7.9259e-02, -1.9145e-02,  8.1228e-02,\n",
       "         -5.8607e-02, -1.1548e-02,  3.0062e-02, -4.2975e-02,  1.7548e-02],\n",
       "        [ 6.2598e-02, -9.8335e-02,  8.1569e-02, -2.2912e-02,  7.8898e-02,\n",
       "         -4.9032e-02, -1.7810e-02,  9.3299e-03, -5.1450e-02,  2.6256e-02],\n",
       "        [ 6.4099e-02, -9.6922e-02,  7.9335e-02, -2.1641e-02,  7.8830e-02,\n",
       "         -5.3474e-02, -1.8891e-02,  5.3554e-03, -4.7084e-02,  1.8807e-02],\n",
       "        [ 5.6290e-02, -1.0233e-01,  7.5711e-02, -2.1923e-02,  8.5409e-02,\n",
       "         -6.0207e-02, -1.2584e-02,  5.2051e-03, -4.9352e-02,  1.4297e-02],\n",
       "        [ 6.6917e-02, -1.1098e-01,  7.7420e-02, -2.4513e-02,  7.7912e-02,\n",
       "         -6.0259e-02, -6.0220e-03,  7.3557e-03, -5.4574e-02,  2.8815e-02],\n",
       "        [ 6.0816e-02, -1.0641e-01,  7.3503e-02, -2.7989e-02,  7.5854e-02,\n",
       "         -5.3105e-02, -2.0717e-02,  6.1294e-03, -4.7722e-02,  1.8692e-02],\n",
       "        [ 5.6204e-02, -1.0373e-01,  7.9284e-02, -2.5851e-02,  7.5688e-02,\n",
       "         -5.7125e-02, -1.1538e-02,  9.3648e-03, -5.3785e-02,  2.4732e-02],\n",
       "        [ 6.1879e-02, -1.0285e-01,  8.1362e-02, -2.8011e-02,  8.0524e-02,\n",
       "         -6.1614e-02, -1.6830e-03,  2.2241e-02, -5.4603e-02,  3.1382e-02],\n",
       "        [ 5.6902e-02, -1.0606e-01,  7.7315e-02, -2.5472e-02,  7.6037e-02,\n",
       "         -5.9693e-02, -1.4799e-02,  1.0200e-02, -5.4858e-02,  2.2616e-02],\n",
       "        [ 5.8129e-02, -1.0811e-01,  8.5292e-02, -1.6436e-02,  7.3473e-02,\n",
       "         -5.2121e-02, -1.7967e-02,  2.5582e-02, -5.0211e-02,  2.3305e-02],\n",
       "        [ 6.2396e-02, -1.1087e-01,  7.3472e-02, -9.4943e-03,  7.8653e-02,\n",
       "         -6.4198e-02,  3.6178e-04,  9.4117e-03, -6.1738e-02,  4.2839e-02],\n",
       "        [ 7.1253e-02, -1.0612e-01,  8.3373e-02, -1.9047e-02,  8.0714e-02,\n",
       "         -5.7693e-02, -1.8429e-03,  1.6270e-02, -5.4453e-02,  2.9577e-02],\n",
       "        [ 5.9544e-02, -1.0559e-01,  8.8489e-02, -3.1565e-02,  8.0020e-02,\n",
       "         -5.5438e-02, -5.7251e-03,  1.8664e-02, -5.2366e-02,  2.3172e-02],\n",
       "        [ 6.1612e-02, -9.9962e-02,  6.8567e-02, -1.1921e-02,  7.7715e-02,\n",
       "         -6.2662e-02, -1.9513e-03,  2.1397e-02, -4.6588e-02,  3.4849e-02],\n",
       "        [ 6.2774e-02, -1.0908e-01,  7.9167e-02, -2.5710e-02,  7.7027e-02,\n",
       "         -5.6526e-02, -5.5923e-03,  2.7524e-02, -5.7672e-02,  3.2633e-02],\n",
       "        [ 6.2831e-02, -9.3318e-02,  7.2774e-02, -1.4149e-02,  7.9027e-02,\n",
       "         -6.6472e-02, -1.0094e-02,  1.6573e-02, -4.3307e-02,  3.2477e-02],\n",
       "        [ 5.4736e-02, -1.0611e-01,  7.9437e-02, -8.5882e-03,  7.8907e-02,\n",
       "         -6.1257e-02,  1.9513e-03,  7.7329e-03, -4.8438e-02,  3.2541e-02],\n",
       "        [ 6.1127e-02, -9.7673e-02,  7.9933e-02, -1.9460e-02,  8.2634e-02,\n",
       "         -5.7866e-02, -1.0499e-03,  1.7480e-02, -5.5722e-02,  3.1606e-02],\n",
       "        [ 6.7872e-02, -1.0577e-01,  8.6769e-02, -1.9790e-02,  7.8594e-02,\n",
       "         -6.3011e-02, -2.4643e-03,  1.8620e-02, -5.5599e-02,  3.1857e-02],\n",
       "        [ 6.7686e-02, -1.0048e-01,  7.8174e-02, -2.5306e-02,  8.0071e-02,\n",
       "         -5.1469e-02, -2.2099e-02,  6.4069e-03, -5.0429e-02,  2.1465e-02],\n",
       "        [ 6.6983e-02, -1.2092e-01,  8.2842e-02, -2.4852e-02,  8.3823e-02,\n",
       "         -5.6960e-02, -4.0768e-03,  9.0682e-03, -5.6647e-02,  2.8945e-02],\n",
       "        [ 6.8026e-02, -1.0006e-01,  8.1237e-02, -1.3136e-02,  7.8220e-02,\n",
       "         -5.5035e-02, -1.6586e-02,  1.5167e-02, -4.9631e-02,  2.3378e-02],\n",
       "        [ 6.2870e-02, -1.0530e-01,  7.8826e-02, -1.7351e-02,  7.8953e-02,\n",
       "         -5.6817e-02, -1.6414e-02, -9.8897e-05, -4.4886e-02,  3.5201e-02],\n",
       "        [ 5.0052e-02, -1.1398e-01,  8.7865e-02, -2.5317e-02,  8.0178e-02,\n",
       "         -4.6712e-02, -5.7932e-03,  1.5101e-02, -6.0397e-02,  2.1109e-02],\n",
       "        [ 6.5806e-02, -9.8005e-02,  8.3194e-02, -2.5516e-02,  8.1393e-02,\n",
       "         -5.6172e-02, -9.2248e-03,  1.0992e-02, -4.7453e-02,  2.0291e-02],\n",
       "        [ 5.3870e-02, -1.0561e-01,  7.7877e-02, -1.8585e-02,  8.3538e-02,\n",
       "         -5.6186e-02, -5.1050e-03,  1.6225e-02, -5.0806e-02,  2.2447e-02]],\n",
       "       device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        # input: 28 * 28\n",
    "        # after first conv with kernel size 5: 24 * 24\n",
    "        # after first pool: 12 * 12\n",
    "        # after second conv with kernel size 5: 8 * 8\n",
    "        # after second pool: 4 * 4\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "X = X.to(device)\n",
    "x = X[0].to(device)\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4d23b29-2043-4840-907e-1691d22cb77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current layer size:  150\n",
      "Current layer size:  6\n",
      "Current layer size:  2400\n",
      "Current layer size:  16\n",
      "Current layer size:  30720\n",
      "Current layer size:  120\n",
      "Current layer size:  10080\n",
      "Current layer size:  84\n",
      "Current layer size:  840\n",
      "Current layer size:  10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44426"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_params = 0\n",
    "for params in model.parameters():\n",
    "    current_layer_size = np.prod(params.shape)\n",
    "    print(\"Current layer size: \", current_layer_size)\n",
    "    total_params += current_layer_size\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3e756c8-4ebc-4590-b864-4e7763166ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc40f675-8988-41b5-be33-dc8dc7e9947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9d982f3-ebce-4b55-9c1c-ac8d70a5e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e2fc992-696e-4059-a466-b237799e794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 9.1%, Avg loss: 2.305011 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303705  [   64/60000]\n",
      "loss: 2.311559  [ 6464/60000]\n",
      "loss: 2.305902  [12864/60000]\n",
      "loss: 2.299397  [19264/60000]\n",
      "loss: 2.290324  [25664/60000]\n",
      "loss: 2.305754  [32064/60000]\n",
      "loss: 2.310317  [38464/60000]\n",
      "loss: 2.308900  [44864/60000]\n",
      "loss: 2.301286  [51264/60000]\n",
      "loss: 2.296918  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 13.5%, Avg loss: 2.300978 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.299212  [   64/60000]\n",
      "loss: 2.306777  [ 6464/60000]\n",
      "loss: 2.302367  [12864/60000]\n",
      "loss: 2.295259  [19264/60000]\n",
      "loss: 2.288781  [25664/60000]\n",
      "loss: 2.302387  [32064/60000]\n",
      "loss: 2.305556  [38464/60000]\n",
      "loss: 2.305829  [44864/60000]\n",
      "loss: 2.297535  [51264/60000]\n",
      "loss: 2.293281  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 15.5%, Avg loss: 2.296882 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.294836  [   64/60000]\n",
      "loss: 2.301764  [ 6464/60000]\n",
      "loss: 2.298210  [12864/60000]\n",
      "loss: 2.291070  [19264/60000]\n",
      "loss: 2.285635  [25664/60000]\n",
      "loss: 2.298275  [32064/60000]\n",
      "loss: 2.300344  [38464/60000]\n",
      "loss: 2.301706  [44864/60000]\n",
      "loss: 2.292941  [51264/60000]\n",
      "loss: 2.289218  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 17.2%, Avg loss: 2.291994 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.289567  [   64/60000]\n",
      "loss: 2.296031  [ 6464/60000]\n",
      "loss: 2.293120  [12864/60000]\n",
      "loss: 2.285848  [19264/60000]\n",
      "loss: 2.280973  [25664/60000]\n",
      "loss: 2.292863  [32064/60000]\n",
      "loss: 2.294056  [38464/60000]\n",
      "loss: 2.296172  [44864/60000]\n",
      "loss: 2.286303  [51264/60000]\n",
      "loss: 2.283457  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 22.9%, Avg loss: 2.284943 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.282317  [   64/60000]\n",
      "loss: 2.288400  [ 6464/60000]\n",
      "loss: 2.285686  [12864/60000]\n",
      "loss: 2.277971  [19264/60000]\n",
      "loss: 2.272663  [25664/60000]\n",
      "loss: 2.284197  [32064/60000]\n",
      "loss: 2.285191  [38464/60000]\n",
      "loss: 2.287791  [44864/60000]\n",
      "loss: 2.275401  [51264/60000]\n",
      "loss: 2.274047  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.273697 \n",
      "\n",
      "Done in  31.931153297424316  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "epochs = 5\n",
    "test(test_dataloader, model, loss_fn)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done in \", time.time() - start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17c0e60c-8301-47d1-a77d-36765e06fabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.273697 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.271137  [   64/60000]\n",
      "loss: 2.276597  [ 6464/60000]\n",
      "loss: 2.272983  [12864/60000]\n",
      "loss: 2.263204  [19264/60000]\n",
      "loss: 2.257730  [25664/60000]\n",
      "loss: 2.269225  [32064/60000]\n",
      "loss: 2.270618  [38464/60000]\n",
      "loss: 2.270711  [44864/60000]\n",
      "loss: 2.254058  [51264/60000]\n",
      "loss: 2.255198  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 2.251399 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.249337  [   64/60000]\n",
      "loss: 2.252872  [ 6464/60000]\n",
      "loss: 2.247308  [12864/60000]\n",
      "loss: 2.233288  [19264/60000]\n",
      "loss: 2.224857  [25664/60000]\n",
      "loss: 2.235671  [32064/60000]\n",
      "loss: 2.239175  [38464/60000]\n",
      "loss: 2.234028  [44864/60000]\n",
      "loss: 2.206384  [51264/60000]\n",
      "loss: 2.213970  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 2.199036 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.198809  [   64/60000]\n",
      "loss: 2.196806  [ 6464/60000]\n",
      "loss: 2.182954  [12864/60000]\n",
      "loss: 2.157520  [19264/60000]\n",
      "loss: 2.131334  [25664/60000]\n",
      "loss: 2.137643  [32064/60000]\n",
      "loss: 2.142460  [38464/60000]\n",
      "loss: 2.113240  [44864/60000]\n",
      "loss: 2.048877  [51264/60000]\n",
      "loss: 2.057963  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 2.011320 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.026596  [   64/60000]\n",
      "loss: 1.986428  [ 6464/60000]\n",
      "loss: 1.915793  [12864/60000]\n",
      "loss: 1.840338  [19264/60000]\n",
      "loss: 1.741433  [25664/60000]\n",
      "loss: 1.724038  [32064/60000]\n",
      "loss: 1.717318  [38464/60000]\n",
      "loss: 1.592249  [44864/60000]\n",
      "loss: 1.484515  [51264/60000]\n",
      "loss: 1.440899  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.332001 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.399894  [   64/60000]\n",
      "loss: 1.257416  [ 6464/60000]\n",
      "loss: 1.141907  [12864/60000]\n",
      "loss: 1.063673  [19264/60000]\n",
      "loss: 1.013629  [25664/60000]\n",
      "loss: 0.959116  [32064/60000]\n",
      "loss: 1.037190  [38464/60000]\n",
      "loss: 0.926703  [44864/60000]\n",
      "loss: 0.990247  [51264/60000]\n",
      "loss: 0.936227  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.795071 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.914259  [   64/60000]\n",
      "loss: 0.758871  [ 6464/60000]\n",
      "loss: 0.710324  [12864/60000]\n",
      "loss: 0.734093  [19264/60000]\n",
      "loss: 0.741520  [25664/60000]\n",
      "loss: 0.663808  [32064/60000]\n",
      "loss: 0.702055  [38464/60000]\n",
      "loss: 0.674483  [44864/60000]\n",
      "loss: 0.774538  [51264/60000]\n",
      "loss: 0.746708  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.602373 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.741999  [   64/60000]\n",
      "loss: 0.590942  [ 6464/60000]\n",
      "loss: 0.537889  [12864/60000]\n",
      "loss: 0.621777  [19264/60000]\n",
      "loss: 0.620380  [25664/60000]\n",
      "loss: 0.547545  [32064/60000]\n",
      "loss: 0.512545  [38464/60000]\n",
      "loss: 0.564937  [44864/60000]\n",
      "loss: 0.641713  [51264/60000]\n",
      "loss: 0.636709  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.500846 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.643829  [   64/60000]\n",
      "loss: 0.500014  [ 6464/60000]\n",
      "loss: 0.435482  [12864/60000]\n",
      "loss: 0.556539  [19264/60000]\n",
      "loss: 0.546728  [25664/60000]\n",
      "loss: 0.482643  [32064/60000]\n",
      "loss: 0.395482  [38464/60000]\n",
      "loss: 0.508578  [44864/60000]\n",
      "loss: 0.553990  [51264/60000]\n",
      "loss: 0.567641  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.438078 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.578304  [   64/60000]\n",
      "loss: 0.440437  [ 6464/60000]\n",
      "loss: 0.367268  [12864/60000]\n",
      "loss: 0.513073  [19264/60000]\n",
      "loss: 0.495781  [25664/60000]\n",
      "loss: 0.437701  [32064/60000]\n",
      "loss: 0.318877  [38464/60000]\n",
      "loss: 0.476171  [44864/60000]\n",
      "loss: 0.487089  [51264/60000]\n",
      "loss: 0.520612  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.394524 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.528230  [   64/60000]\n",
      "loss: 0.393851  [ 6464/60000]\n",
      "loss: 0.317663  [12864/60000]\n",
      "loss: 0.478864  [19264/60000]\n",
      "loss: 0.455474  [25664/60000]\n",
      "loss: 0.399354  [32064/60000]\n",
      "loss: 0.265018  [38464/60000]\n",
      "loss: 0.451536  [44864/60000]\n",
      "loss: 0.432522  [51264/60000]\n",
      "loss: 0.480722  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.361279 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.482900  [   64/60000]\n",
      "loss: 0.361400  [ 6464/60000]\n",
      "loss: 0.277460  [12864/60000]\n",
      "loss: 0.449192  [19264/60000]\n",
      "loss: 0.420220  [25664/60000]\n",
      "loss: 0.368871  [32064/60000]\n",
      "loss: 0.224629  [38464/60000]\n",
      "loss: 0.433196  [44864/60000]\n",
      "loss: 0.388647  [51264/60000]\n",
      "loss: 0.448346  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.334745 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.444481  [   64/60000]\n",
      "loss: 0.338038  [ 6464/60000]\n",
      "loss: 0.245584  [12864/60000]\n",
      "loss: 0.426536  [19264/60000]\n",
      "loss: 0.387808  [25664/60000]\n",
      "loss: 0.342154  [32064/60000]\n",
      "loss: 0.194671  [38464/60000]\n",
      "loss: 0.416963  [44864/60000]\n",
      "loss: 0.353735  [51264/60000]\n",
      "loss: 0.418783  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.312726 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.410757  [   64/60000]\n",
      "loss: 0.319925  [ 6464/60000]\n",
      "loss: 0.220878  [12864/60000]\n",
      "loss: 0.407345  [19264/60000]\n",
      "loss: 0.357755  [25664/60000]\n",
      "loss: 0.319942  [32064/60000]\n",
      "loss: 0.172613  [38464/60000]\n",
      "loss: 0.402724  [44864/60000]\n",
      "loss: 0.326506  [51264/60000]\n",
      "loss: 0.391614  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.293716 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.379086  [   64/60000]\n",
      "loss: 0.303941  [ 6464/60000]\n",
      "loss: 0.201397  [12864/60000]\n",
      "loss: 0.391407  [19264/60000]\n",
      "loss: 0.331212  [25664/60000]\n",
      "loss: 0.300541  [32064/60000]\n",
      "loss: 0.156432  [38464/60000]\n",
      "loss: 0.387955  [44864/60000]\n",
      "loss: 0.303357  [51264/60000]\n",
      "loss: 0.366322  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.277183 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.349433  [   64/60000]\n",
      "loss: 0.289967  [ 6464/60000]\n",
      "loss: 0.185913  [12864/60000]\n",
      "loss: 0.378490  [19264/60000]\n",
      "loss: 0.307079  [25664/60000]\n",
      "loss: 0.284111  [32064/60000]\n",
      "loss: 0.144182  [38464/60000]\n",
      "loss: 0.374236  [44864/60000]\n",
      "loss: 0.284751  [51264/60000]\n",
      "loss: 0.344155  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.262619 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.321642  [   64/60000]\n",
      "loss: 0.277662  [ 6464/60000]\n",
      "loss: 0.172749  [12864/60000]\n",
      "loss: 0.366381  [19264/60000]\n",
      "loss: 0.286221  [25664/60000]\n",
      "loss: 0.270446  [32064/60000]\n",
      "loss: 0.134697  [38464/60000]\n",
      "loss: 0.360943  [44864/60000]\n",
      "loss: 0.268583  [51264/60000]\n",
      "loss: 0.324872  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.249754 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.296540  [   64/60000]\n",
      "loss: 0.266387  [ 6464/60000]\n",
      "loss: 0.161817  [12864/60000]\n",
      "loss: 0.355795  [19264/60000]\n",
      "loss: 0.267060  [25664/60000]\n",
      "loss: 0.257801  [32064/60000]\n",
      "loss: 0.126979  [38464/60000]\n",
      "loss: 0.348429  [44864/60000]\n",
      "loss: 0.255296  [51264/60000]\n",
      "loss: 0.307342  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.238162 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.274387  [   64/60000]\n",
      "loss: 0.255998  [ 6464/60000]\n",
      "loss: 0.153387  [12864/60000]\n",
      "loss: 0.345699  [19264/60000]\n",
      "loss: 0.249956  [25664/60000]\n",
      "loss: 0.245856  [32064/60000]\n",
      "loss: 0.121193  [38464/60000]\n",
      "loss: 0.336857  [44864/60000]\n",
      "loss: 0.244867  [51264/60000]\n",
      "loss: 0.291926  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.227775 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.254755  [   64/60000]\n",
      "loss: 0.245946  [ 6464/60000]\n",
      "loss: 0.146328  [12864/60000]\n",
      "loss: 0.337024  [19264/60000]\n",
      "loss: 0.234190  [25664/60000]\n",
      "loss: 0.234957  [32064/60000]\n",
      "loss: 0.116468  [38464/60000]\n",
      "loss: 0.326537  [44864/60000]\n",
      "loss: 0.236320  [51264/60000]\n",
      "loss: 0.278922  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.218197 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.237016  [   64/60000]\n",
      "loss: 0.236233  [ 6464/60000]\n",
      "loss: 0.140333  [12864/60000]\n",
      "loss: 0.329648  [19264/60000]\n",
      "loss: 0.220823  [25664/60000]\n",
      "loss: 0.225299  [32064/60000]\n",
      "loss: 0.112972  [38464/60000]\n",
      "loss: 0.317471  [44864/60000]\n",
      "loss: 0.229428  [51264/60000]\n",
      "loss: 0.267951  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.209461 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.220705  [   64/60000]\n",
      "loss: 0.227228  [ 6464/60000]\n",
      "loss: 0.135517  [12864/60000]\n",
      "loss: 0.322935  [19264/60000]\n",
      "loss: 0.208522  [25664/60000]\n",
      "loss: 0.216839  [32064/60000]\n",
      "loss: 0.109847  [38464/60000]\n",
      "loss: 0.308789  [44864/60000]\n",
      "loss: 0.223629  [51264/60000]\n",
      "loss: 0.258921  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.201474 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.205925  [   64/60000]\n",
      "loss: 0.218732  [ 6464/60000]\n",
      "loss: 0.131303  [12864/60000]\n",
      "loss: 0.316049  [19264/60000]\n",
      "loss: 0.196479  [25664/60000]\n",
      "loss: 0.209146  [32064/60000]\n",
      "loss: 0.107123  [38464/60000]\n",
      "loss: 0.301490  [44864/60000]\n",
      "loss: 0.219091  [51264/60000]\n",
      "loss: 0.250638  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.194085 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.192645  [   64/60000]\n",
      "loss: 0.210647  [ 6464/60000]\n",
      "loss: 0.127306  [12864/60000]\n",
      "loss: 0.308769  [19264/60000]\n",
      "loss: 0.185969  [25664/60000]\n",
      "loss: 0.203244  [32064/60000]\n",
      "loss: 0.105739  [38464/60000]\n",
      "loss: 0.295307  [44864/60000]\n",
      "loss: 0.216294  [51264/60000]\n",
      "loss: 0.243841  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.187379 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.180583  [   64/60000]\n",
      "loss: 0.203754  [ 6464/60000]\n",
      "loss: 0.123715  [12864/60000]\n",
      "loss: 0.302054  [19264/60000]\n",
      "loss: 0.175824  [25664/60000]\n",
      "loss: 0.197151  [32064/60000]\n",
      "loss: 0.105068  [38464/60000]\n",
      "loss: 0.289694  [44864/60000]\n",
      "loss: 0.214465  [51264/60000]\n",
      "loss: 0.238505  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.181028 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.169960  [   64/60000]\n",
      "loss: 0.197555  [ 6464/60000]\n",
      "loss: 0.120519  [12864/60000]\n",
      "loss: 0.295636  [19264/60000]\n",
      "loss: 0.166340  [25664/60000]\n",
      "loss: 0.191118  [32064/60000]\n",
      "loss: 0.104728  [38464/60000]\n",
      "loss: 0.284598  [44864/60000]\n",
      "loss: 0.212964  [51264/60000]\n",
      "loss: 0.233837  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.175092 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.160351  [   64/60000]\n",
      "loss: 0.191927  [ 6464/60000]\n",
      "loss: 0.118164  [12864/60000]\n",
      "loss: 0.289200  [19264/60000]\n",
      "loss: 0.157757  [25664/60000]\n",
      "loss: 0.185216  [32064/60000]\n",
      "loss: 0.104904  [38464/60000]\n",
      "loss: 0.279352  [44864/60000]\n",
      "loss: 0.212677  [51264/60000]\n",
      "loss: 0.230616  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.9%, Avg loss: 0.169486 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.151682  [   64/60000]\n",
      "loss: 0.187133  [ 6464/60000]\n",
      "loss: 0.116298  [12864/60000]\n",
      "loss: 0.283112  [19264/60000]\n",
      "loss: 0.149796  [25664/60000]\n",
      "loss: 0.179667  [32064/60000]\n",
      "loss: 0.105363  [38464/60000]\n",
      "loss: 0.274787  [44864/60000]\n",
      "loss: 0.212639  [51264/60000]\n",
      "loss: 0.227613  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.164343 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.143858  [   64/60000]\n",
      "loss: 0.182906  [ 6464/60000]\n",
      "loss: 0.114310  [12864/60000]\n",
      "loss: 0.276451  [19264/60000]\n",
      "loss: 0.142299  [25664/60000]\n",
      "loss: 0.174832  [32064/60000]\n",
      "loss: 0.105715  [38464/60000]\n",
      "loss: 0.270158  [44864/60000]\n",
      "loss: 0.212556  [51264/60000]\n",
      "loss: 0.225135  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.159523 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.136969  [   64/60000]\n",
      "loss: 0.179258  [ 6464/60000]\n",
      "loss: 0.112904  [12864/60000]\n",
      "loss: 0.270862  [19264/60000]\n",
      "loss: 0.135468  [25664/60000]\n",
      "loss: 0.169211  [32064/60000]\n",
      "loss: 0.106118  [38464/60000]\n",
      "loss: 0.265206  [44864/60000]\n",
      "loss: 0.212872  [51264/60000]\n",
      "loss: 0.222608  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.154923 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.130543  [   64/60000]\n",
      "loss: 0.175036  [ 6464/60000]\n",
      "loss: 0.111640  [12864/60000]\n",
      "loss: 0.264900  [19264/60000]\n",
      "loss: 0.128540  [25664/60000]\n",
      "loss: 0.163800  [32064/60000]\n",
      "loss: 0.106349  [38464/60000]\n",
      "loss: 0.260955  [44864/60000]\n",
      "loss: 0.213125  [51264/60000]\n",
      "loss: 0.220483  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.150493 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.123413  [   64/60000]\n",
      "loss: 0.170555  [ 6464/60000]\n",
      "loss: 0.110886  [12864/60000]\n",
      "loss: 0.259433  [19264/60000]\n",
      "loss: 0.122390  [25664/60000]\n",
      "loss: 0.158092  [32064/60000]\n",
      "loss: 0.107202  [38464/60000]\n",
      "loss: 0.256392  [44864/60000]\n",
      "loss: 0.211986  [51264/60000]\n",
      "loss: 0.217132  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.146231 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.117511  [   64/60000]\n",
      "loss: 0.166531  [ 6464/60000]\n",
      "loss: 0.109916  [12864/60000]\n",
      "loss: 0.253924  [19264/60000]\n",
      "loss: 0.115982  [25664/60000]\n",
      "loss: 0.152122  [32064/60000]\n",
      "loss: 0.107953  [38464/60000]\n",
      "loss: 0.252030  [44864/60000]\n",
      "loss: 0.211771  [51264/60000]\n",
      "loss: 0.213820  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.7%, Avg loss: 0.142315 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.111848  [   64/60000]\n",
      "loss: 0.162276  [ 6464/60000]\n",
      "loss: 0.109054  [12864/60000]\n",
      "loss: 0.248106  [19264/60000]\n",
      "loss: 0.110567  [25664/60000]\n",
      "loss: 0.146936  [32064/60000]\n",
      "loss: 0.108983  [38464/60000]\n",
      "loss: 0.247074  [44864/60000]\n",
      "loss: 0.212720  [51264/60000]\n",
      "loss: 0.211775  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.138865 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.106516  [   64/60000]\n",
      "loss: 0.159355  [ 6464/60000]\n",
      "loss: 0.108022  [12864/60000]\n",
      "loss: 0.242159  [19264/60000]\n",
      "loss: 0.104729  [25664/60000]\n",
      "loss: 0.142263  [32064/60000]\n",
      "loss: 0.109907  [38464/60000]\n",
      "loss: 0.242822  [44864/60000]\n",
      "loss: 0.213830  [51264/60000]\n",
      "loss: 0.209869  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.135543 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.102033  [   64/60000]\n",
      "loss: 0.156057  [ 6464/60000]\n",
      "loss: 0.107250  [12864/60000]\n",
      "loss: 0.236120  [19264/60000]\n",
      "loss: 0.099661  [25664/60000]\n",
      "loss: 0.137313  [32064/60000]\n",
      "loss: 0.110536  [38464/60000]\n",
      "loss: 0.237604  [44864/60000]\n",
      "loss: 0.215165  [51264/60000]\n",
      "loss: 0.208038  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.132390 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.097963  [   64/60000]\n",
      "loss: 0.153315  [ 6464/60000]\n",
      "loss: 0.106312  [12864/60000]\n",
      "loss: 0.229476  [19264/60000]\n",
      "loss: 0.095003  [25664/60000]\n",
      "loss: 0.132838  [32064/60000]\n",
      "loss: 0.111342  [38464/60000]\n",
      "loss: 0.232955  [44864/60000]\n",
      "loss: 0.216414  [51264/60000]\n",
      "loss: 0.206118  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.129419 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.094432  [   64/60000]\n",
      "loss: 0.150857  [ 6464/60000]\n",
      "loss: 0.105034  [12864/60000]\n",
      "loss: 0.223796  [19264/60000]\n",
      "loss: 0.090403  [25664/60000]\n",
      "loss: 0.129344  [32064/60000]\n",
      "loss: 0.112058  [38464/60000]\n",
      "loss: 0.228950  [44864/60000]\n",
      "loss: 0.217417  [51264/60000]\n",
      "loss: 0.204825  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.126668 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.090886  [   64/60000]\n",
      "loss: 0.148647  [ 6464/60000]\n",
      "loss: 0.104521  [12864/60000]\n",
      "loss: 0.218452  [19264/60000]\n",
      "loss: 0.085924  [25664/60000]\n",
      "loss: 0.126122  [32064/60000]\n",
      "loss: 0.112915  [38464/60000]\n",
      "loss: 0.225568  [44864/60000]\n",
      "loss: 0.218415  [51264/60000]\n",
      "loss: 0.204236  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.1%, Avg loss: 0.124001 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.087102  [   64/60000]\n",
      "loss: 0.146014  [ 6464/60000]\n",
      "loss: 0.104188  [12864/60000]\n",
      "loss: 0.214150  [19264/60000]\n",
      "loss: 0.081929  [25664/60000]\n",
      "loss: 0.123371  [32064/60000]\n",
      "loss: 0.113581  [38464/60000]\n",
      "loss: 0.221145  [44864/60000]\n",
      "loss: 0.218816  [51264/60000]\n",
      "loss: 0.203079  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 0.121597 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.084215  [   64/60000]\n",
      "loss: 0.143848  [ 6464/60000]\n",
      "loss: 0.103602  [12864/60000]\n",
      "loss: 0.210887  [19264/60000]\n",
      "loss: 0.078293  [25664/60000]\n",
      "loss: 0.120108  [32064/60000]\n",
      "loss: 0.114152  [38464/60000]\n",
      "loss: 0.217681  [44864/60000]\n",
      "loss: 0.219643  [51264/60000]\n",
      "loss: 0.201901  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.119406 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.081580  [   64/60000]\n",
      "loss: 0.142012  [ 6464/60000]\n",
      "loss: 0.103133  [12864/60000]\n",
      "loss: 0.208051  [19264/60000]\n",
      "loss: 0.074517  [25664/60000]\n",
      "loss: 0.117121  [32064/60000]\n",
      "loss: 0.114997  [38464/60000]\n",
      "loss: 0.213966  [44864/60000]\n",
      "loss: 0.220472  [51264/60000]\n",
      "loss: 0.200471  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.117264 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.079191  [   64/60000]\n",
      "loss: 0.140320  [ 6464/60000]\n",
      "loss: 0.102578  [12864/60000]\n",
      "loss: 0.205155  [19264/60000]\n",
      "loss: 0.070899  [25664/60000]\n",
      "loss: 0.114607  [32064/60000]\n",
      "loss: 0.115844  [38464/60000]\n",
      "loss: 0.210556  [44864/60000]\n",
      "loss: 0.221227  [51264/60000]\n",
      "loss: 0.199616  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.115135 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.077107  [   64/60000]\n",
      "loss: 0.138852  [ 6464/60000]\n",
      "loss: 0.101920  [12864/60000]\n",
      "loss: 0.202154  [19264/60000]\n",
      "loss: 0.067551  [25664/60000]\n",
      "loss: 0.112076  [32064/60000]\n",
      "loss: 0.116986  [38464/60000]\n",
      "loss: 0.207533  [44864/60000]\n",
      "loss: 0.221675  [51264/60000]\n",
      "loss: 0.197768  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.113131 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.074790  [   64/60000]\n",
      "loss: 0.137192  [ 6464/60000]\n",
      "loss: 0.101726  [12864/60000]\n",
      "loss: 0.198892  [19264/60000]\n",
      "loss: 0.064001  [25664/60000]\n",
      "loss: 0.109387  [32064/60000]\n",
      "loss: 0.117814  [38464/60000]\n",
      "loss: 0.204408  [44864/60000]\n",
      "loss: 0.222684  [51264/60000]\n",
      "loss: 0.196368  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.111252 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.072990  [   64/60000]\n",
      "loss: 0.136206  [ 6464/60000]\n",
      "loss: 0.101623  [12864/60000]\n",
      "loss: 0.196458  [19264/60000]\n",
      "loss: 0.060863  [25664/60000]\n",
      "loss: 0.107523  [32064/60000]\n",
      "loss: 0.118452  [38464/60000]\n",
      "loss: 0.201101  [44864/60000]\n",
      "loss: 0.223369  [51264/60000]\n",
      "loss: 0.195360  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.109584 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.071343  [   64/60000]\n",
      "loss: 0.135725  [ 6464/60000]\n",
      "loss: 0.101353  [12864/60000]\n",
      "loss: 0.193464  [19264/60000]\n",
      "loss: 0.057960  [25664/60000]\n",
      "loss: 0.105075  [32064/60000]\n",
      "loss: 0.118975  [38464/60000]\n",
      "loss: 0.198260  [44864/60000]\n",
      "loss: 0.223659  [51264/60000]\n",
      "loss: 0.193562  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.108008 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.069707  [   64/60000]\n",
      "loss: 0.134640  [ 6464/60000]\n",
      "loss: 0.101371  [12864/60000]\n",
      "loss: 0.190942  [19264/60000]\n",
      "loss: 0.055599  [25664/60000]\n",
      "loss: 0.102751  [32064/60000]\n",
      "loss: 0.118826  [38464/60000]\n",
      "loss: 0.196103  [44864/60000]\n",
      "loss: 0.224309  [51264/60000]\n",
      "loss: 0.192257  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.106510 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.068357  [   64/60000]\n",
      "loss: 0.133822  [ 6464/60000]\n",
      "loss: 0.101514  [12864/60000]\n",
      "loss: 0.188590  [19264/60000]\n",
      "loss: 0.053322  [25664/60000]\n",
      "loss: 0.100506  [32064/60000]\n",
      "loss: 0.118840  [38464/60000]\n",
      "loss: 0.194109  [44864/60000]\n",
      "loss: 0.224692  [51264/60000]\n",
      "loss: 0.190734  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.105000 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.066648  [   64/60000]\n",
      "loss: 0.132826  [ 6464/60000]\n",
      "loss: 0.101710  [12864/60000]\n",
      "loss: 0.186458  [19264/60000]\n",
      "loss: 0.050942  [25664/60000]\n",
      "loss: 0.098410  [32064/60000]\n",
      "loss: 0.118670  [38464/60000]\n",
      "loss: 0.191483  [44864/60000]\n",
      "loss: 0.225233  [51264/60000]\n",
      "loss: 0.189837  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.103603 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.064984  [   64/60000]\n",
      "loss: 0.131720  [ 6464/60000]\n",
      "loss: 0.101987  [12864/60000]\n",
      "loss: 0.184073  [19264/60000]\n",
      "loss: 0.048792  [25664/60000]\n",
      "loss: 0.096373  [32064/60000]\n",
      "loss: 0.118740  [38464/60000]\n",
      "loss: 0.189562  [44864/60000]\n",
      "loss: 0.225566  [51264/60000]\n",
      "loss: 0.188246  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.102196 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.063494  [   64/60000]\n",
      "loss: 0.131198  [ 6464/60000]\n",
      "loss: 0.101951  [12864/60000]\n",
      "loss: 0.181985  [19264/60000]\n",
      "loss: 0.046918  [25664/60000]\n",
      "loss: 0.094357  [32064/60000]\n",
      "loss: 0.118318  [38464/60000]\n",
      "loss: 0.187723  [44864/60000]\n",
      "loss: 0.225442  [51264/60000]\n",
      "loss: 0.186884  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.100889 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.061713  [   64/60000]\n",
      "loss: 0.130313  [ 6464/60000]\n",
      "loss: 0.102168  [12864/60000]\n",
      "loss: 0.179480  [19264/60000]\n",
      "loss: 0.045186  [25664/60000]\n",
      "loss: 0.092408  [32064/60000]\n",
      "loss: 0.118056  [38464/60000]\n",
      "loss: 0.186331  [44864/60000]\n",
      "loss: 0.225475  [51264/60000]\n",
      "loss: 0.185419  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.099527 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.060278  [   64/60000]\n",
      "loss: 0.130210  [ 6464/60000]\n",
      "loss: 0.102072  [12864/60000]\n",
      "loss: 0.176900  [19264/60000]\n",
      "loss: 0.043538  [25664/60000]\n",
      "loss: 0.090614  [32064/60000]\n",
      "loss: 0.117552  [38464/60000]\n",
      "loss: 0.184219  [44864/60000]\n",
      "loss: 0.225396  [51264/60000]\n",
      "loss: 0.184206  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.098430 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.059260  [   64/60000]\n",
      "loss: 0.129399  [ 6464/60000]\n",
      "loss: 0.102053  [12864/60000]\n",
      "loss: 0.174857  [19264/60000]\n",
      "loss: 0.041928  [25664/60000]\n",
      "loss: 0.088573  [32064/60000]\n",
      "loss: 0.117130  [38464/60000]\n",
      "loss: 0.182322  [44864/60000]\n",
      "loss: 0.225488  [51264/60000]\n",
      "loss: 0.182949  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.097281 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.058066  [   64/60000]\n",
      "loss: 0.128610  [ 6464/60000]\n",
      "loss: 0.101902  [12864/60000]\n",
      "loss: 0.172516  [19264/60000]\n",
      "loss: 0.040359  [25664/60000]\n",
      "loss: 0.086720  [32064/60000]\n",
      "loss: 0.117112  [38464/60000]\n",
      "loss: 0.180391  [44864/60000]\n",
      "loss: 0.225674  [51264/60000]\n",
      "loss: 0.181621  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.096221 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.056774  [   64/60000]\n",
      "loss: 0.128235  [ 6464/60000]\n",
      "loss: 0.101848  [12864/60000]\n",
      "loss: 0.170643  [19264/60000]\n",
      "loss: 0.039005  [25664/60000]\n",
      "loss: 0.085525  [32064/60000]\n",
      "loss: 0.116925  [38464/60000]\n",
      "loss: 0.178519  [44864/60000]\n",
      "loss: 0.224809  [51264/60000]\n",
      "loss: 0.179799  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.095147 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.055817  [   64/60000]\n",
      "loss: 0.127949  [ 6464/60000]\n",
      "loss: 0.102116  [12864/60000]\n",
      "loss: 0.168705  [19264/60000]\n",
      "loss: 0.037620  [25664/60000]\n",
      "loss: 0.083905  [32064/60000]\n",
      "loss: 0.116799  [38464/60000]\n",
      "loss: 0.176763  [44864/60000]\n",
      "loss: 0.224805  [51264/60000]\n",
      "loss: 0.178281  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.094152 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.054800  [   64/60000]\n",
      "loss: 0.127714  [ 6464/60000]\n",
      "loss: 0.101878  [12864/60000]\n",
      "loss: 0.166313  [19264/60000]\n",
      "loss: 0.036436  [25664/60000]\n",
      "loss: 0.082324  [32064/60000]\n",
      "loss: 0.116630  [38464/60000]\n",
      "loss: 0.175587  [44864/60000]\n",
      "loss: 0.224634  [51264/60000]\n",
      "loss: 0.176546  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.093166 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.053855  [   64/60000]\n",
      "loss: 0.127656  [ 6464/60000]\n",
      "loss: 0.101829  [12864/60000]\n",
      "loss: 0.163828  [19264/60000]\n",
      "loss: 0.035467  [25664/60000]\n",
      "loss: 0.080420  [32064/60000]\n",
      "loss: 0.115935  [38464/60000]\n",
      "loss: 0.174600  [44864/60000]\n",
      "loss: 0.224022  [51264/60000]\n",
      "loss: 0.175344  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.092184 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.052933  [   64/60000]\n",
      "loss: 0.127332  [ 6464/60000]\n",
      "loss: 0.101642  [12864/60000]\n",
      "loss: 0.162098  [19264/60000]\n",
      "loss: 0.034464  [25664/60000]\n",
      "loss: 0.079067  [32064/60000]\n",
      "loss: 0.115465  [38464/60000]\n",
      "loss: 0.173341  [44864/60000]\n",
      "loss: 0.223923  [51264/60000]\n",
      "loss: 0.173989  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.091282 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.052106  [   64/60000]\n",
      "loss: 0.127028  [ 6464/60000]\n",
      "loss: 0.101356  [12864/60000]\n",
      "loss: 0.160048  [19264/60000]\n",
      "loss: 0.033425  [25664/60000]\n",
      "loss: 0.077543  [32064/60000]\n",
      "loss: 0.114889  [38464/60000]\n",
      "loss: 0.172270  [44864/60000]\n",
      "loss: 0.223430  [51264/60000]\n",
      "loss: 0.173332  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.090385 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.051142  [   64/60000]\n",
      "loss: 0.126524  [ 6464/60000]\n",
      "loss: 0.101274  [12864/60000]\n",
      "loss: 0.157940  [19264/60000]\n",
      "loss: 0.032439  [25664/60000]\n",
      "loss: 0.075997  [32064/60000]\n",
      "loss: 0.114510  [38464/60000]\n",
      "loss: 0.170977  [44864/60000]\n",
      "loss: 0.222996  [51264/60000]\n",
      "loss: 0.172087  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.089565 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.050276  [   64/60000]\n",
      "loss: 0.126201  [ 6464/60000]\n",
      "loss: 0.101476  [12864/60000]\n",
      "loss: 0.155461  [19264/60000]\n",
      "loss: 0.031462  [25664/60000]\n",
      "loss: 0.074921  [32064/60000]\n",
      "loss: 0.113902  [38464/60000]\n",
      "loss: 0.169532  [44864/60000]\n",
      "loss: 0.222184  [51264/60000]\n",
      "loss: 0.171244  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.088722 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.049437  [   64/60000]\n",
      "loss: 0.125635  [ 6464/60000]\n",
      "loss: 0.101067  [12864/60000]\n",
      "loss: 0.153955  [19264/60000]\n",
      "loss: 0.030627  [25664/60000]\n",
      "loss: 0.073432  [32064/60000]\n",
      "loss: 0.113542  [38464/60000]\n",
      "loss: 0.168163  [44864/60000]\n",
      "loss: 0.221486  [51264/60000]\n",
      "loss: 0.170354  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.087936 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.048699  [   64/60000]\n",
      "loss: 0.125395  [ 6464/60000]\n",
      "loss: 0.100729  [12864/60000]\n",
      "loss: 0.152722  [19264/60000]\n",
      "loss: 0.029610  [25664/60000]\n",
      "loss: 0.072078  [32064/60000]\n",
      "loss: 0.113279  [38464/60000]\n",
      "loss: 0.166799  [44864/60000]\n",
      "loss: 0.220705  [51264/60000]\n",
      "loss: 0.169688  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.087228 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.048138  [   64/60000]\n",
      "loss: 0.125162  [ 6464/60000]\n",
      "loss: 0.100243  [12864/60000]\n",
      "loss: 0.151023  [19264/60000]\n",
      "loss: 0.028716  [25664/60000]\n",
      "loss: 0.070900  [32064/60000]\n",
      "loss: 0.112681  [38464/60000]\n",
      "loss: 0.165290  [44864/60000]\n",
      "loss: 0.219564  [51264/60000]\n",
      "loss: 0.168710  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.086475 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.047278  [   64/60000]\n",
      "loss: 0.124844  [ 6464/60000]\n",
      "loss: 0.099907  [12864/60000]\n",
      "loss: 0.149549  [19264/60000]\n",
      "loss: 0.027936  [25664/60000]\n",
      "loss: 0.069270  [32064/60000]\n",
      "loss: 0.111882  [38464/60000]\n",
      "loss: 0.164350  [44864/60000]\n",
      "loss: 0.218386  [51264/60000]\n",
      "loss: 0.167643  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.085777 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.046670  [   64/60000]\n",
      "loss: 0.124241  [ 6464/60000]\n",
      "loss: 0.099754  [12864/60000]\n",
      "loss: 0.147981  [19264/60000]\n",
      "loss: 0.027084  [25664/60000]\n",
      "loss: 0.067962  [32064/60000]\n",
      "loss: 0.111033  [38464/60000]\n",
      "loss: 0.163131  [44864/60000]\n",
      "loss: 0.218277  [51264/60000]\n",
      "loss: 0.166708  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.085093 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.045738  [   64/60000]\n",
      "loss: 0.123726  [ 6464/60000]\n",
      "loss: 0.099606  [12864/60000]\n",
      "loss: 0.146910  [19264/60000]\n",
      "loss: 0.026332  [25664/60000]\n",
      "loss: 0.066795  [32064/60000]\n",
      "loss: 0.110471  [38464/60000]\n",
      "loss: 0.161758  [44864/60000]\n",
      "loss: 0.217453  [51264/60000]\n",
      "loss: 0.165965  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.084412 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.044968  [   64/60000]\n",
      "loss: 0.123311  [ 6464/60000]\n",
      "loss: 0.099052  [12864/60000]\n",
      "loss: 0.145586  [19264/60000]\n",
      "loss: 0.025665  [25664/60000]\n",
      "loss: 0.065408  [32064/60000]\n",
      "loss: 0.109701  [38464/60000]\n",
      "loss: 0.160821  [44864/60000]\n",
      "loss: 0.216748  [51264/60000]\n",
      "loss: 0.164946  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.083736 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.044264  [   64/60000]\n",
      "loss: 0.122941  [ 6464/60000]\n",
      "loss: 0.098732  [12864/60000]\n",
      "loss: 0.144383  [19264/60000]\n",
      "loss: 0.025052  [25664/60000]\n",
      "loss: 0.064225  [32064/60000]\n",
      "loss: 0.109044  [38464/60000]\n",
      "loss: 0.159777  [44864/60000]\n",
      "loss: 0.215655  [51264/60000]\n",
      "loss: 0.163894  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.083060 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.043671  [   64/60000]\n",
      "loss: 0.122630  [ 6464/60000]\n",
      "loss: 0.098902  [12864/60000]\n",
      "loss: 0.142994  [19264/60000]\n",
      "loss: 0.024495  [25664/60000]\n",
      "loss: 0.062892  [32064/60000]\n",
      "loss: 0.108360  [38464/60000]\n",
      "loss: 0.158976  [44864/60000]\n",
      "loss: 0.215182  [51264/60000]\n",
      "loss: 0.162794  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.082376 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.043012  [   64/60000]\n",
      "loss: 0.122229  [ 6464/60000]\n",
      "loss: 0.098699  [12864/60000]\n",
      "loss: 0.141420  [19264/60000]\n",
      "loss: 0.023983  [25664/60000]\n",
      "loss: 0.061575  [32064/60000]\n",
      "loss: 0.107807  [38464/60000]\n",
      "loss: 0.158087  [44864/60000]\n",
      "loss: 0.214325  [51264/60000]\n",
      "loss: 0.161442  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.081806 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.042538  [   64/60000]\n",
      "loss: 0.121518  [ 6464/60000]\n",
      "loss: 0.098322  [12864/60000]\n",
      "loss: 0.140254  [19264/60000]\n",
      "loss: 0.023483  [25664/60000]\n",
      "loss: 0.060423  [32064/60000]\n",
      "loss: 0.107620  [38464/60000]\n",
      "loss: 0.156787  [44864/60000]\n",
      "loss: 0.213582  [51264/60000]\n",
      "loss: 0.160439  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.081195 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.041998  [   64/60000]\n",
      "loss: 0.121253  [ 6464/60000]\n",
      "loss: 0.098158  [12864/60000]\n",
      "loss: 0.138424  [19264/60000]\n",
      "loss: 0.022957  [25664/60000]\n",
      "loss: 0.059331  [32064/60000]\n",
      "loss: 0.107252  [38464/60000]\n",
      "loss: 0.155927  [44864/60000]\n",
      "loss: 0.212597  [51264/60000]\n",
      "loss: 0.159334  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.080532 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.041427  [   64/60000]\n",
      "loss: 0.120486  [ 6464/60000]\n",
      "loss: 0.098030  [12864/60000]\n",
      "loss: 0.136436  [19264/60000]\n",
      "loss: 0.022535  [25664/60000]\n",
      "loss: 0.057964  [32064/60000]\n",
      "loss: 0.106719  [38464/60000]\n",
      "loss: 0.154535  [44864/60000]\n",
      "loss: 0.211250  [51264/60000]\n",
      "loss: 0.158254  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.079867 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.040904  [   64/60000]\n",
      "loss: 0.119830  [ 6464/60000]\n",
      "loss: 0.097815  [12864/60000]\n",
      "loss: 0.135056  [19264/60000]\n",
      "loss: 0.022051  [25664/60000]\n",
      "loss: 0.056699  [32064/60000]\n",
      "loss: 0.106345  [38464/60000]\n",
      "loss: 0.153318  [44864/60000]\n",
      "loss: 0.210577  [51264/60000]\n",
      "loss: 0.157162  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.079258 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.040395  [   64/60000]\n",
      "loss: 0.119089  [ 6464/60000]\n",
      "loss: 0.097241  [12864/60000]\n",
      "loss: 0.133345  [19264/60000]\n",
      "loss: 0.021624  [25664/60000]\n",
      "loss: 0.055566  [32064/60000]\n",
      "loss: 0.105662  [38464/60000]\n",
      "loss: 0.152224  [44864/60000]\n",
      "loss: 0.209454  [51264/60000]\n",
      "loss: 0.156034  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.078638 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.039736  [   64/60000]\n",
      "loss: 0.118582  [ 6464/60000]\n",
      "loss: 0.096606  [12864/60000]\n",
      "loss: 0.131681  [19264/60000]\n",
      "loss: 0.021252  [25664/60000]\n",
      "loss: 0.053973  [32064/60000]\n",
      "loss: 0.104789  [38464/60000]\n",
      "loss: 0.151315  [44864/60000]\n",
      "loss: 0.208667  [51264/60000]\n",
      "loss: 0.155100  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.078061 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.039086  [   64/60000]\n",
      "loss: 0.118022  [ 6464/60000]\n",
      "loss: 0.096291  [12864/60000]\n",
      "loss: 0.130114  [19264/60000]\n",
      "loss: 0.020897  [25664/60000]\n",
      "loss: 0.053160  [32064/60000]\n",
      "loss: 0.104362  [38464/60000]\n",
      "loss: 0.149986  [44864/60000]\n",
      "loss: 0.207913  [51264/60000]\n",
      "loss: 0.153563  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.077501 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.038456  [   64/60000]\n",
      "loss: 0.117455  [ 6464/60000]\n",
      "loss: 0.095890  [12864/60000]\n",
      "loss: 0.128231  [19264/60000]\n",
      "loss: 0.020563  [25664/60000]\n",
      "loss: 0.052019  [32064/60000]\n",
      "loss: 0.103822  [38464/60000]\n",
      "loss: 0.148922  [44864/60000]\n",
      "loss: 0.206797  [51264/60000]\n",
      "loss: 0.152425  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.076915 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.037967  [   64/60000]\n",
      "loss: 0.117045  [ 6464/60000]\n",
      "loss: 0.095199  [12864/60000]\n",
      "loss: 0.126630  [19264/60000]\n",
      "loss: 0.020242  [25664/60000]\n",
      "loss: 0.050945  [32064/60000]\n",
      "loss: 0.103127  [38464/60000]\n",
      "loss: 0.147811  [44864/60000]\n",
      "loss: 0.205178  [51264/60000]\n",
      "loss: 0.151259  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.076350 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.037400  [   64/60000]\n",
      "loss: 0.116642  [ 6464/60000]\n",
      "loss: 0.094562  [12864/60000]\n",
      "loss: 0.125014  [19264/60000]\n",
      "loss: 0.019918  [25664/60000]\n",
      "loss: 0.049626  [32064/60000]\n",
      "loss: 0.102591  [38464/60000]\n",
      "loss: 0.146854  [44864/60000]\n",
      "loss: 0.203523  [51264/60000]\n",
      "loss: 0.150232  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.075805 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.036763  [   64/60000]\n",
      "loss: 0.116105  [ 6464/60000]\n",
      "loss: 0.093915  [12864/60000]\n",
      "loss: 0.123555  [19264/60000]\n",
      "loss: 0.019646  [25664/60000]\n",
      "loss: 0.048208  [32064/60000]\n",
      "loss: 0.101896  [38464/60000]\n",
      "loss: 0.145888  [44864/60000]\n",
      "loss: 0.201303  [51264/60000]\n",
      "loss: 0.149247  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.075252 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.036429  [   64/60000]\n",
      "loss: 0.115983  [ 6464/60000]\n",
      "loss: 0.093376  [12864/60000]\n",
      "loss: 0.122370  [19264/60000]\n",
      "loss: 0.019237  [25664/60000]\n",
      "loss: 0.046943  [32064/60000]\n",
      "loss: 0.101206  [38464/60000]\n",
      "loss: 0.145258  [44864/60000]\n",
      "loss: 0.199850  [51264/60000]\n",
      "loss: 0.148513  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.074655 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.035793  [   64/60000]\n",
      "loss: 0.115527  [ 6464/60000]\n",
      "loss: 0.092639  [12864/60000]\n",
      "loss: 0.121065  [19264/60000]\n",
      "loss: 0.019017  [25664/60000]\n",
      "loss: 0.045857  [32064/60000]\n",
      "loss: 0.100514  [38464/60000]\n",
      "loss: 0.144731  [44864/60000]\n",
      "loss: 0.198397  [51264/60000]\n",
      "loss: 0.147724  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.074171 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.035320  [   64/60000]\n",
      "loss: 0.115128  [ 6464/60000]\n",
      "loss: 0.091800  [12864/60000]\n",
      "loss: 0.119676  [19264/60000]\n",
      "loss: 0.018590  [25664/60000]\n",
      "loss: 0.044624  [32064/60000]\n",
      "loss: 0.100132  [38464/60000]\n",
      "loss: 0.143948  [44864/60000]\n",
      "loss: 0.196827  [51264/60000]\n",
      "loss: 0.146932  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.073669 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.034858  [   64/60000]\n",
      "loss: 0.114808  [ 6464/60000]\n",
      "loss: 0.090861  [12864/60000]\n",
      "loss: 0.117974  [19264/60000]\n",
      "loss: 0.018374  [25664/60000]\n",
      "loss: 0.043619  [32064/60000]\n",
      "loss: 0.099637  [38464/60000]\n",
      "loss: 0.143147  [44864/60000]\n",
      "loss: 0.195307  [51264/60000]\n",
      "loss: 0.146482  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.073184 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.034453  [   64/60000]\n",
      "loss: 0.114475  [ 6464/60000]\n",
      "loss: 0.089812  [12864/60000]\n",
      "loss: 0.116444  [19264/60000]\n",
      "loss: 0.018061  [25664/60000]\n",
      "loss: 0.042514  [32064/60000]\n",
      "loss: 0.099135  [38464/60000]\n",
      "loss: 0.142632  [44864/60000]\n",
      "loss: 0.193799  [51264/60000]\n",
      "loss: 0.145375  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.072732 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.033973  [   64/60000]\n",
      "loss: 0.114185  [ 6464/60000]\n",
      "loss: 0.089089  [12864/60000]\n",
      "loss: 0.115340  [19264/60000]\n",
      "loss: 0.017854  [25664/60000]\n",
      "loss: 0.041702  [32064/60000]\n",
      "loss: 0.098504  [38464/60000]\n",
      "loss: 0.141915  [44864/60000]\n",
      "loss: 0.191729  [51264/60000]\n",
      "loss: 0.144930  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.072202 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.033403  [   64/60000]\n",
      "loss: 0.113949  [ 6464/60000]\n",
      "loss: 0.088560  [12864/60000]\n",
      "loss: 0.114049  [19264/60000]\n",
      "loss: 0.017481  [25664/60000]\n",
      "loss: 0.040723  [32064/60000]\n",
      "loss: 0.097997  [38464/60000]\n",
      "loss: 0.140954  [44864/60000]\n",
      "loss: 0.190148  [51264/60000]\n",
      "loss: 0.143725  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.071753 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.033225  [   64/60000]\n",
      "loss: 0.113406  [ 6464/60000]\n",
      "loss: 0.087804  [12864/60000]\n",
      "loss: 0.112526  [19264/60000]\n",
      "loss: 0.017103  [25664/60000]\n",
      "loss: 0.039867  [32064/60000]\n",
      "loss: 0.097531  [38464/60000]\n",
      "loss: 0.140556  [44864/60000]\n",
      "loss: 0.188515  [51264/60000]\n",
      "loss: 0.142134  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.071341 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.032792  [   64/60000]\n",
      "loss: 0.113018  [ 6464/60000]\n",
      "loss: 0.087237  [12864/60000]\n",
      "loss: 0.111338  [19264/60000]\n",
      "loss: 0.016987  [25664/60000]\n",
      "loss: 0.039175  [32064/60000]\n",
      "loss: 0.096962  [38464/60000]\n",
      "loss: 0.139608  [44864/60000]\n",
      "loss: 0.186873  [51264/60000]\n",
      "loss: 0.141373  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.070907 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.032332  [   64/60000]\n",
      "loss: 0.112469  [ 6464/60000]\n",
      "loss: 0.086398  [12864/60000]\n",
      "loss: 0.110103  [19264/60000]\n",
      "loss: 0.016699  [25664/60000]\n",
      "loss: 0.038219  [32064/60000]\n",
      "loss: 0.096509  [38464/60000]\n",
      "loss: 0.138903  [44864/60000]\n",
      "loss: 0.185376  [51264/60000]\n",
      "loss: 0.140201  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.070444 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.031940  [   64/60000]\n",
      "loss: 0.112048  [ 6464/60000]\n",
      "loss: 0.086053  [12864/60000]\n",
      "loss: 0.108770  [19264/60000]\n",
      "loss: 0.016465  [25664/60000]\n",
      "loss: 0.037474  [32064/60000]\n",
      "loss: 0.095925  [38464/60000]\n",
      "loss: 0.138252  [44864/60000]\n",
      "loss: 0.183775  [51264/60000]\n",
      "loss: 0.139375  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.070034 \n",
      "\n",
      "Done in  547.3503620624542  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "epochs = 95\n",
    "test(test_dataloader, model, loss_fn)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done in \", time.time() - start, \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
